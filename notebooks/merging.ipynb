{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc7f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".dataframe td, .dataframe th {\n",
       "    white-space: nowrap !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import gzip\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td, .dataframe th {\n",
    "    white-space: nowrap !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490fd441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "# root = os.path.dirname(os.path.abspath(__file__))\n",
    "root = \".\"\n",
    "sys.path.append(os.path.join(root, \"..\", \"src\"))\n",
    "from default import DATAPATH, CONFIGPATH\n",
    "\n",
    "# Load pathogen info\n",
    "# pathogen_code = sys.argv[1]\n",
    "pathogen_code = 'smansoni'\n",
    "df = pd.read_csv(os.path.join(CONFIGPATH, 'pathogens.csv'))\n",
    "row = df.loc[df[\"code\"].eq(pathogen_code)]\n",
    "if row.empty: \n",
    "    raise SystemExit(f\"Unknown code: {pathogen_code}\")\n",
    "pathogen = row.iloc[0][\"pathogen\"]\n",
    "\n",
    "print(\"Step 14\")\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT = os.path.join(root, \"..\", \"output\")\n",
    "\n",
    "# Shared columns\n",
    "KEYS = [\"assay_id\", \"activity_type\", \"unit\"]\n",
    "\n",
    "# Columns to take from datasets table\n",
    "COLUMNS_DATASETS = [\"equal\", 'higher', 'lower', \"target_type_curated_extra\", \"dataset_type\", \"cpds_qt\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4612192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_gz_csvs_from_zip(zip_path):\n",
    "    \"\"\"Read all ``*.csv.gz`` members from a ZIP into DataFrames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zip_path : str | pathlib.Path\n",
    "        Path to the ZIP archive.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, pandas.DataFrame]\n",
    "        Mapping of ZIP member name -> loaded DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for name in z.namelist():\n",
    "            if name.endswith(\".csv.gz\"):\n",
    "                with z.open(name) as f:\n",
    "                    dfs[name] = pd.read_csv(f, compression=\"gzip\")\n",
    "    return dfs\n",
    "\n",
    "def get_all_results_from_individual_modeling(INDIVIDUAL_LM, LABELS=['A', 'B', 'C', 'D']):\n",
    "    \"\"\"Collect best AUROC (>0.7) per (assay_id, activity_type, unit) for each label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RESULTS : dict[str, dict[tuple, list]]\n",
    "        Per label: (assay_id, activity_type, unit) -> [expert_cutoff, best_auroc],\n",
    "        considering only rows with AUROC > 0.7.\n",
    "    CONSIDERED_ASSAYS : dict[str, set[tuple]]\n",
    "        Per label: set of all (assay_id, activity_type, unit) keys encountered\n",
    "        (no AUROC threshold applied).\n",
    "    \"\"\"\n",
    "    RESULTS, CONSIDERED_ASSAYS = {}, {}\n",
    "    for LABEL in LABELS:\n",
    "        RESULTS[LABEL] = {}\n",
    "        CONSIDERED_ASSAYS[LABEL] = set()\n",
    "        rows = INDIVIDUAL_LM[INDIVIDUAL_LM[LABEL]][[\"assay_id\", \"activity_type\", \"unit\", \"expert_cutoff\", f\"{LABEL}_AVG\"]].values\n",
    "        for assay_id, activity_type, unit, expert_cutoff, auroc in rows:\n",
    "            key = (assay_id, activity_type, unit)\n",
    "            CONSIDERED_ASSAYS[LABEL].add(key)\n",
    "            if auroc > 0.7:\n",
    "                if key not in RESULTS[LABEL]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "                elif auroc > RESULTS[LABEL][key][1]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "    return RESULTS, CONSIDERED_ASSAYS\n",
    "\n",
    "def where_considered(key, LABELS, CONSIDERED_ASSAYS):\n",
    "    \"\"\"Return labels (semicolon-separated) where `key` was considered; else NaN.\"\"\"\n",
    "    considered = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in CONSIDERED_ASSAYS[LABEL]:\n",
    "            considered.append(LABEL)\n",
    "    if len(considered) > 0:\n",
    "        return \";\".join(considered)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def where_accepted(key, LABELS, ACCEPTED_ASSAYS):\n",
    "    \"\"\"Return labels (semicolon-separated) where `key` was accepted; else NaN.\"\"\"\n",
    "    accepted = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in ACCEPTED_ASSAYS[LABEL]:\n",
    "            accepted.append(LABEL)\n",
    "    if len(accepted) > 0:\n",
    "        return \";\".join(accepted)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_filtered_assay_master_organism(assay_df, activity_type, unit, direction, assay_type, target_type_curated_extra, bao_label, strain):\n",
    "    \"\"\"Filter `assay_df` by metadata fields, treating non-string `unit` as missing (NaN).\"\"\"\n",
    "    if type(unit) == str:\n",
    "        if type(strain) == str:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'] == unit) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'] == strain)]\n",
    "        else:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'] == unit) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'].isna())]\n",
    "    else:\n",
    "        if type(strain) == str:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'].isna()) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'] == strain)]\n",
    "        else:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'].isna()) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'].isna())]\n",
    "    return df\n",
    "\n",
    "def get_filtered_assay_master_single_protein(assay_df, activity_type, unit, direction, assay_type, target_type_curated_extra, bao_label, strain, target_chembl_id):\n",
    "    \"\"\"Filter `assay_df` by metadata fields, treating non-string `unit` as missing (NaN).\"\"\"\n",
    "    if type(unit) == str:\n",
    "        if type(strain) == str:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'] == unit) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'] == strain) & \n",
    "                        (assay_df['target_chembl_id'] == target_chembl_id)]\n",
    "        else:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'] == unit) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'].isna()) & \n",
    "                         (assay_df['target_chembl_id'] == target_chembl_id)]\n",
    "    else:\n",
    "        if type(strain) == str:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'].isna()) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'] == strain) & \n",
    "                        (assay_df['target_chembl_id'] == target_chembl_id)]\n",
    "        else:\n",
    "            df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                        (assay_df['unit'].isna()) &\n",
    "                        (assay_df['direction'] == direction) &\n",
    "                        (assay_df['assay_type'] == assay_type) &\n",
    "                        (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                        (assay_df['bao_label'] == bao_label) &\n",
    "                        (assay_df['strain'].isna()) & \n",
    "                        (assay_df['target_chembl_id'] == target_chembl_id)]\n",
    "    return df\n",
    "\n",
    "def load_expert_cutoffs(CONFIGPATH):\n",
    "    \"\"\"\n",
    "    Load expert cutoffs from the manual curation CSV and return them as a dictionary.\n",
    "\n",
    "    The CSV is expected at:\n",
    "        {CONFIGPATH}/manual_curation/expert_cutoffs.csv\n",
    "\n",
    "    The returned dictionary maps:\n",
    "        (activity_type, unit, target_type, pathogen_code) -> expert_cutoff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    CONFIGPATH : str\n",
    "        Path to the config folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of expert cutoffs keyed by\n",
    "        (activity_type, unit, target_type, pathogen_code).\n",
    "    \"\"\"\n",
    "    # Load expert cut-offs\n",
    "    EXPERT_CUTOFFS = pd.read_csv(os.path.join(CONFIGPATH, \"expert_cutoffs.csv\"))\n",
    "\n",
    "    EXPERT_CUTOFFS = {\n",
    "        (a, b, c, d): [float(k) for k in e.split(\";\")]\n",
    "        for a, b, c, d, e in EXPERT_CUTOFFS[\n",
    "            [\"activity_type\", \"unit\", \"target_type\", \"pathogen_code\", \"expert_cutoff\"]\n",
    "        ].values\n",
    "    }\n",
    "\n",
    "    return EXPERT_CUTOFFS\n",
    "\n",
    "def load_ecfp_all(h5_path):\n",
    "    \"\"\"Load all ECFP (Morgan count) fingerprints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing datasets \"SMILES\" and \"X_morgan\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, np.ndarray]\n",
    "        Mapping {chembl_id: fingerprint (np.int8, shape (nBits,))}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        meta = f[\"SMILES\"][:, 3].astype(str)\n",
    "        fps  = f[\"X_morgan\"][:]  # Load ALL\n",
    "\n",
    "    return {cid: fp for cid, fp in zip(meta, fps)}\n",
    "\n",
    "def KFoldTrain(X, Y, n_splits=4, n_estimators=100, random_state=42):\n",
    "    \"\"\"Stratified K-fold training/eval with RandomForest; returns mean AUROC and std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Binary labels (n_samples,).\n",
    "    n_splits : int\n",
    "        Number of folds.\n",
    "    n_estimators : int\n",
    "        Number of trees in the random forest.\n",
    "    random_state : int\n",
    "        RNG seed (also used for fold shuffling).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        (mean_auroc, std_auroc) rounded to 3 decimals.\n",
    "    \"\"\"\n",
    "    def init_RF():\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aurocs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = init_RF()\n",
    "        rf.fit(X_train, Y_train)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "        aurocs.append(roc_auc_score(Y_test, y_prob))\n",
    "\n",
    "    return round(float(np.mean(aurocs)), 3), round(float(np.std(aurocs)), 3)\n",
    "\n",
    "def TrainRF(X, Y, n_estimators=100):\n",
    "    \"\"\"Train a RandomForestClassifier on all provided data and return the fitted model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Labels (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RandomForestClassifier\n",
    "        Fitted classifier.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    rf.fit(X, Y)\n",
    "    return rf\n",
    "\n",
    "def to_merge_unique_cpds(df, group_keys, assay_to_compounds):\n",
    "    \"\"\"\n",
    "    Group assays by `group_keys` and compute:\n",
    "      - n_assays: number of unique assays in the group\n",
    "      - n_cpds_union: number of unique compounds across assays (set union)\n",
    "      - assay_keys: ';'-separated tuple strings \"(assay_id, activity_type, unit)\" (last column)\n",
    "    \"\"\"\n",
    "\n",
    "    def collect_assay_keys(block):\n",
    "        \"\"\"Return unique (assay_id, activity_type, unit) keys for this group.\"\"\"\n",
    "        keys = sorted({tuple(r) for r in block.values})\n",
    "        return keys  # list[tuple]\n",
    "\n",
    "    def union_size(keys):\n",
    "        \"\"\"Return size of union of compounds for the given assay keys.\"\"\"\n",
    "        u = set()\n",
    "        for k in keys:\n",
    "            u |= assay_to_compounds.get(k, set())\n",
    "        return len(u)\n",
    "\n",
    "    out = (df.groupby(group_keys, dropna=False)[[\"assay_id\", \"activity_type\", \"unit\"]]\n",
    "             .apply(collect_assay_keys)\n",
    "             .reset_index(name=\"assay_keys\"))\n",
    "\n",
    "    out[\"n_assays\"] = out[\"assay_keys\"].apply(len)\n",
    "    out[\"n_cpds_union\"] = out[\"assay_keys\"].apply(union_size)\n",
    "\n",
    "    # store as ';'-separated tuple strings (easy round-trip via ast.literal_eval)\n",
    "    out[\"assay_keys\"] = out[\"assay_keys\"].apply(lambda ks: \";\".join(map(str, ks)))\n",
    "\n",
    "    # make assay_keys the last column\n",
    "    cols = [c for c in out.columns if c != \"assay_keys\"] + [\"assay_keys\"]\n",
    "    out = out[cols]\n",
    "\n",
    "    return out.sort_values(\"n_cpds_union\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def get_target_chembl_id(merging, target_type):\n",
    "    if target_type == 'SINGLE PROTEIN':\n",
    "        return merging.target_chembl_id\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "RATIO = 0.1\n",
    "\n",
    "# Set and create path to correlations\n",
    "PATH_TO_CORRELATIONS = os.path.join(OUTPUT, pathogen_code, \"correlations\")\n",
    "os.makedirs(os.path.join(PATH_TO_CORRELATIONS, \"M\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3110ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging assay metadata\n",
      "Mapping assays to compounds\n",
      "Loading individual datasets\n",
      "Loaded quantitative: 1338 datasets\n",
      "Loaded qualitative: 345 datasets\n",
      "Loading ECFPs...\n",
      "Loading reference set of compounds\n",
      "Defining decoys\n"
     ]
    }
   ],
   "source": [
    "# Load assays info\n",
    "print(\"Merging assay metadata\")\n",
    "ASSAYS_CLEANED = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_cleaned.csv\"))\n",
    "ASSAYS_PARAMETERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_parameters.csv\"))\n",
    "ASSAYS_DATASETS_ = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_datasets.csv\"))\n",
    "INDIVIDUAL_LM = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"individual_LM.csv\"))\n",
    "\n",
    "# Get assay to quantitative data info to collapse ASSAY_DATASETS_ (1 row per assay)\n",
    "assay_to_qt_info = defaultdict(list)\n",
    "for assay_id, activity_type, unit, expert_cutoff, ratio_qt in ASSAYS_DATASETS_[['assay_id', 'activity_type', 'unit', 'expert_cutoff', 'ratio_qt']].values:\n",
    "    assay_to_qt_info[tuple([assay_id, activity_type, unit])].append([expert_cutoff, ratio_qt])\n",
    "\n",
    "# Unique row per assay\n",
    "ASSAYS_DATASETS = ASSAYS_DATASETS_[KEYS + COLUMNS_DATASETS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Get cutoffs and ratios\n",
    "cutoffs = [\";\".join([str(j[0]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "ratios = [\";\".join([str(j[1]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "cutoffs = [i if i != 'nan' else np.nan for i in cutoffs]\n",
    "ratios = [i if i != 'nan' else np.nan for i in ratios]\n",
    "\n",
    "# Store results\n",
    "ASSAYS_DATASETS.insert(8, 'cutoffs', cutoffs)\n",
    "ASSAYS_DATASETS.insert(9, 'ratios', ratios)\n",
    "\n",
    "# Merge everything\n",
    "ASSAYS_MASTER = ASSAYS_CLEANED.merge(ASSAYS_PARAMETERS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "ASSAYS_MASTER = ASSAYS_MASTER.merge(ASSAYS_DATASETS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Dict mapping assay_id, activity_type and unit to a set of compound ChEMBL IDs\n",
    "print(\"Mapping assays to compounds\")\n",
    "ChEMBL = pd.read_csv(os.path.join(OUTPUT, pathogen_code, f\"{pathogen_code}_ChEMBL_cleaned_data.csv.gz\"), low_memory=False)\n",
    "ASSAY_TO_COMPOUNDS = defaultdict(set)\n",
    "for assay_id, activity_type, unit, compound_chembl_id in ChEMBL[[\"assay_chembl_id\", \"activity_type\", \"unit\", \"compound_chembl_id\"]].values:\n",
    "    ASSAY_TO_COMPOUNDS[(assay_id, activity_type, unit)].add(compound_chembl_id)\n",
    "del ChEMBL\n",
    "\n",
    "# Loading quantitative and qualitative datasets\n",
    "print(\"Loading individual datasets\")\n",
    "qt_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_qt.zip\")\n",
    "ql_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_ql.zip\")\n",
    "dfs_qt = load_all_gz_csvs_from_zip(qt_zip)\n",
    "dfs_ql = load_all_gz_csvs_from_zip(ql_zip)\n",
    "print(\"Loaded quantitative:\", len(dfs_qt), \"datasets\")\n",
    "print(\"Loaded qualitative:\", len(dfs_ql), \"datasets\")\n",
    "\n",
    "# Get results from individual modeling ABCD\n",
    "LABELS = ['A', 'B', 'C', 'D']\n",
    "ACCEPTED_ASSAYS, CONSIDERED_ASSAYS = get_all_results_from_individual_modeling(INDIVIDUAL_LM, LABELS)\n",
    "\n",
    "col_accepted, col_considered = [], []\n",
    "for assay_id, activity_type, unit in ASSAYS_MASTER[[\"assay_id\", \"activity_type\", \"unit\"]].values:\n",
    "    # Get strategies in which this assay is considered and accepted\n",
    "    key = tuple([assay_id, activity_type, unit])\n",
    "    col_considered.append(where_considered(key, LABELS, CONSIDERED_ASSAYS))\n",
    "    col_accepted.append(where_accepted(key, LABELS, ACCEPTED_ASSAYS))\n",
    "ASSAYS_MASTER['Accepted'] = col_accepted\n",
    "ASSAYS_MASTER['Considered'] = col_considered\n",
    "\n",
    "# Reorder columns\n",
    "ALL_COLS = [\"assay_id\", \"assay_type\", \"assay_organism\", \"target_organism\", \"organism_curated\", \"doc_chembl_id\", \"target_type\", \"target_type_curated\", \"target_type_curated_extra\", \n",
    "          \"target_chembl_id\", \"target_chembl_id_curated\", \"target_name_curated\", \"bao_label\", \"source_label\", \"strain\", \"atcc_id\", \"mutations\", \"known_drug_resistances\", \"media\",\n",
    "          \"activity_type\", \"unit\", \"activities\", \"nan_values\", \"cpds\", \"frac_cs\", \"direction\", \"act_flag\", 'inact_flag', \"equal\", \"higher\", \"lower\", \"dataset_type\", \"cutoffs\", \"ratios\", \n",
    "          \"cpds_qt\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", 'Accepted', 'Considered']\n",
    "ASSAYS_MASTER = ASSAYS_MASTER[ALL_COLS]\n",
    "\n",
    "# Get accepted assays and accepted compounds in ABCD\n",
    "accepted_assays = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna() == False)][['assay_id', 'activity_type', 'unit']].values\n",
    "accepted_compounds = set([j for i in accepted_assays for j in ASSAY_TO_COMPOUNDS[tuple(i)]])\n",
    "\n",
    "# Loading Morgan fingerprints\n",
    "print(\"Loading ECFPs...\")\n",
    "PATH_TO_ECFPs = os.path.join(DATAPATH, \"chembl_processed\", \"ChEMBL_ECFPs.h5\")\n",
    "ecfps = load_ecfp_all(PATH_TO_ECFPs)\n",
    "\n",
    "# Loading Reference set of compounds\n",
    "print(\"Loading reference set of compounds\")\n",
    "REFERENCE_SET = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"reference_set.csv.gz\"))['reference_compounds'].tolist()\n",
    "\n",
    "# Prepare reference matrix of Morgan fingerprints\n",
    "X_REF = np.array([ecfps[cid] for cid in REFERENCE_SET if cid in ecfps])\n",
    "\n",
    "# Get all compounds from pathogen\n",
    "compounds = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"compound_counts.csv.gz\"))\n",
    "compounds = set(compounds['compound_chembl_id'])\n",
    "\n",
    "# Get ChEMBL compounds not tested against the pathogen\n",
    "print(\"Defining decoys\")\n",
    "DECOYS_CHEMBL = set([i for i in ecfps if i not in compounds])\n",
    "\n",
    "# Load expert cut-offs\n",
    "EXPERT_CUTOFFS = load_expert_cutoffs(CONFIGPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4588e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying potential assays to merge\n",
      "Organisms...\n",
      "Single proteins...\n"
     ]
    }
   ],
   "source": [
    "# Filtering assays\n",
    "print(\"Identifying potential assays to merge\")\n",
    "print(\"Organisms...\")\n",
    "keys_organism = [\"activity_type\", \"unit\", \"direction\", \"assay_type\", \"target_type_curated_extra\", \"bao_label\", \"strain\"]\n",
    "FILTERED_ASSAYS_ORGANISM = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna()) & (ASSAYS_MASTER['target_type_curated_extra'] == 'ORGANISM')].copy()\n",
    "TO_MERGE_ORGANISM = to_merge_unique_cpds(FILTERED_ASSAYS_ORGANISM, keys_organism, ASSAY_TO_COMPOUNDS)\n",
    "\n",
    "print(\"Single proteins...\")\n",
    "keys_single_protein = [\"activity_type\", \"unit\", \"direction\", \"assay_type\", \"target_type_curated_extra\", \"bao_label\", \"strain\", 'target_chembl_id']\n",
    "FILTERED_ASSAYS_SINGLE_PROTEIN = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna()) & (ASSAYS_MASTER['target_type_curated_extra'] == 'SINGLE PROTEIN')].copy()\n",
    "TO_MERGE_SINGLE_PROTEIN = to_merge_unique_cpds(FILTERED_ASSAYS_SINGLE_PROTEIN, keys_single_protein, ASSAY_TO_COMPOUNDS)\n",
    "\n",
    "# Filtering only activity type - unit pairs relevant for merging\n",
    "TO_MERGE_ORGANISM['name'] = [f\"M_O{r}\" for r in range(len(TO_MERGE_ORGANISM))]\n",
    "TO_MERGE_SINGLE_PROTEIN['name'] = [f\"M_SP{r}\" for r in range(len(TO_MERGE_SINGLE_PROTEIN))]\n",
    "TO_MERGE_ORGANISM = TO_MERGE_ORGANISM[(TO_MERGE_ORGANISM['n_cpds_union'] > 1000) & (TO_MERGE_ORGANISM['n_assays'] > 1)].reset_index(drop=True)\n",
    "TO_MERGE_SINGLE_PROTEIN = TO_MERGE_SINGLE_PROTEIN[(TO_MERGE_SINGLE_PROTEIN['n_cpds_union'] > 1000) & \n",
    "                                                  (TO_MERGE_SINGLE_PROTEIN['n_assays'] > 1) &\n",
    "                                                  (TO_MERGE_SINGLE_PROTEIN['target_chembl_id'].isna() == False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "581f3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANISM\n",
      "SINGLE PROTEIN\n"
     ]
    }
   ],
   "source": [
    "MERGED_COMPOUNDS = []\n",
    "MERGED_LM = []\n",
    "DATA = {\"ORGANISM\": TO_MERGE_ORGANISM, \"SINGLE PROTEIN\": TO_MERGE_SINGLE_PROTEIN}\n",
    "\n",
    "for target_type in DATA:\n",
    "\n",
    "    print(target_type)\n",
    "\n",
    "    # Copy df\n",
    "    data_target_type = DATA[target_type].copy()\n",
    "\n",
    "    # Iterate over activity_type, unit\n",
    "    for merging in data_target_type.itertuples():\n",
    "\n",
    "        # Get data\n",
    "        activity_type = merging.activity_type\n",
    "        unit = merging.unit\n",
    "        direction = float(merging.direction)\n",
    "        assay_type = merging.assay_type\n",
    "        target_type_curated_extra = merging.target_type_curated_extra\n",
    "        bao_label = merging.bao_label\n",
    "        strain = merging.strain\n",
    "        target_chembl_id = get_target_chembl_id(merging, target_type)\n",
    "        name = merging.name\n",
    "        assay_keys = merging.assay_keys\n",
    "        n_assays = merging.n_assays\n",
    "        n_cpds_union = merging.n_cpds_union\n",
    "\n",
    "        # Filter master table\n",
    "        if target_type == 'ORGANISM':\n",
    "            df = get_filtered_assay_master_organism(FILTERED_ASSAYS_ORGANISM, activity_type, unit, direction, assay_type, target_type_curated_extra, bao_label, strain)\n",
    "        elif target_type == 'SINGLE PROTEIN':\n",
    "            df = get_filtered_assay_master_single_protein(FILTERED_ASSAYS_SINGLE_PROTEIN, activity_type, unit, direction, assay_type, target_type_curated_extra, bao_label, strain, target_chembl_id)\n",
    "\n",
    "        # Get quantitative and qualitative\n",
    "        df_quant = df[(df['dataset_type'] == 'quantitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "        df_qual = df[(df['dataset_type'] == 'qualitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "\n",
    "        if len(df_quant) > 0:\n",
    "\n",
    "            # QUANTITATIVE\n",
    "            # For each expert cut-off\n",
    "            for expert_cutoff in EXPERT_CUTOFFS[(activity_type, unit, target_type_curated_extra, pathogen_code)]:\n",
    "                \n",
    "                # Concatenate all files/data together\n",
    "                assays = df_quant['assay_id'].tolist()\n",
    "                files = [f\"{i}_{activity_type}_{unit}_qt_{expert_cutoff}.csv.gz\" for i in assays]\n",
    "                data = [dfs_qt[f].assign(assay_id=a) for a, f in zip(assays, files)]\n",
    "                data = pd.concat(data, ignore_index=True)\n",
    "                if direction == -1:\n",
    "                    data = data.sort_values(\"value\", ascending=True).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "                else:\n",
    "                    data = data.sort_values(\"value\", ascending=False).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "                \n",
    "                # Prepare matrices for training\n",
    "                X = np.array(data['compound_chembl_id'].map(ecfps).to_list())\n",
    "                Y = np.array(data['bin'].tolist())\n",
    "                positives = sum(Y)\n",
    "\n",
    "                if positives > 50:\n",
    "\n",
    "                    print(f\"Merging ... Activity type: {activity_type}, Unit: {unit}, Cutoff: {expert_cutoff}, Strain {strain}, Target ChEMBL ID ({target_chembl_id})\")\n",
    "                    print(f\"\\tTarget ChEMBL ID for ORGANISM assays is set to nan for simplicity\")\n",
    "                    print(f\"\\tCompounds: {len(X)}\", f\"Positives: {positives} ({round(100 * positives / len(Y), 1)}%)\")\n",
    "\n",
    "                    if positives / len(Y) > 0.5:\n",
    "\n",
    "                        print(f\"\\tRatio too high: Adding random compounds from ChEMBL as decoys\")\n",
    "                        DECOYS = int(positives / RATIO - (len(Y) - 1))\n",
    "                        print(f\"\\t{DECOYS} added decoys\")\n",
    "                        rng = random.Random(42)\n",
    "                        DECOYS = rng.sample(list(DECOYS_CHEMBL), DECOYS)\n",
    "                        X_decoys = np.array([ecfps[i] for i in DECOYS])\n",
    "                        X = np.vstack([X, X_decoys])\n",
    "                        Y = np.concatenate([Y, np.zeros(len(X_decoys), dtype=Y.dtype)])\n",
    "                        positives = sum(Y)\n",
    "                        print(f\"\\tCompounds: {len(X)}\", f\"Positives: {positives} ({round(100 * positives / len(Y),3)}%)\")\n",
    "\n",
    "                    # 4Fold Cros Validation\n",
    "                    average_auroc, stds = KFoldTrain(X, Y, n_splits=4, n_estimators=100)\n",
    "                    print(f\"\\tMean AUROC: {average_auroc} ± {stds}\")\n",
    "                    MERGED_LM.append([name, activity_type, unit, expert_cutoff, direction, assay_type, target_type_curated_extra, bao_label, strain, \n",
    "                                      target_chembl_id, n_assays, n_cpds_union, positives, round(positives/len(Y), 3), average_auroc, stds, assay_keys])\n",
    "                    if average_auroc > 0.7:\n",
    "                        MERGED_COMPOUNDS.extend(data['compound_chembl_id'].tolist())\n",
    "                        RF = TrainRF(X, Y, n_estimators=100)\n",
    "                        y_prob_ref = RF.predict_proba(X_REF)[:, 1]\n",
    "                        filename = f'{name}_ref_probs.npz' \n",
    "                        np.savez_compressed(os.path.join(PATH_TO_CORRELATIONS, \"M\", filename), y_prob_ref=y_prob_ref)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Too few positive compounds for {activity_type}_{unit}_qt_{expert_cutoff}.. {strain} ... ({positives})\")\n",
    "\n",
    "        elif len(df_qual) > 0:\n",
    "\n",
    "            # QUALITATIVE\n",
    "            # Concatenate all files/data together\n",
    "            assays = df_qual['assay_id'].tolist()\n",
    "            files = [f\"{i}_{activity_type}_{unit}_ql.csv.gz\" for i in assays]\n",
    "            data = [dfs_ql[f].assign(assay_id=a) for a, f in zip(assays, files)]\n",
    "            data = pd.concat(data, ignore_index=True)\n",
    "            if direction == -1:\n",
    "                data = data.sort_values(\"value\", ascending=True).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "            else:\n",
    "                data = data.sort_values(\"value\", ascending=False).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "            # Prepare matrices for training\n",
    "            X = np.array(data['compound_chembl_id'].map(ecfps).to_list())\n",
    "            Y = np.array(data['bin'].tolist())\n",
    "            positives = sum(Y)\n",
    "\n",
    "            if positives > 50:\n",
    "\n",
    "                print(f\"Merging ... Activity type: {activity_type}, Unit: {unit}, Cutoff: {expert_cutoff}, Strain {strain}, Target ChEMBL ID ({target_chembl_id})\")\n",
    "                print(f\"\\tTarget ChEMBL ID for ORGANISM assays is set to nan for simplicity\")\n",
    "                print(f\"\\tCompounds: {len(X)}\", f\"Positives: {positives} ({round(100 * positives / len(Y), 1)}%)\")\n",
    "\n",
    "                if positives / len(Y) > 0.5:\n",
    "\n",
    "                    print(f\"\\tRatio too high: Adding random compounds from ChEMBL as decoys\")\n",
    "                    DECOYS = int(positives / RATIO - (len(Y) - 1))\n",
    "                    print(f\"\\t{DECOYS} added decoys\")\n",
    "                    rng = random.Random(42)\n",
    "                    DECOYS = rng.sample(list(DECOYS_CHEMBL), DECOYS)\n",
    "                    X_decoys = np.array([ecfps[i] for i in DECOYS])\n",
    "                    X = np.vstack([X, X_decoys])\n",
    "                    Y = np.concatenate([Y, np.zeros(len(X_decoys), dtype=Y.dtype)])\n",
    "                    positives = sum(Y)\n",
    "                    print(f\"\\tCompounds: {len(X)}\", f\"Positives: {positives} ({round(100 * positives / len(Y),3)}%)\")\n",
    "\n",
    "                # 4Fold Cros Validation\n",
    "                average_auroc, stds = KFoldTrain(X, Y, n_splits=5, n_estimators=100)\n",
    "                print(f\"\\tMean AUROC: {average_auroc} ± {stds}\")\n",
    "                # In qualitative datasets, expert cutoff is nan\n",
    "                MERGED_LM.append([name, activity_type, unit, np.nan, direction, assay_type, target_type_curated_extra, bao_label, strain, \n",
    "                                      target_chembl_id, n_assays, n_cpds_union, positives, round(positives/len(Y), 3), average_auroc, stds, assay_keys])\n",
    "                if average_auroc > 0.7:\n",
    "                    MERGED_COMPOUNDS.extend(data['compound_chembl_id'].tolist())\n",
    "                    RF = TrainRF(X, Y, n_estimators=100)\n",
    "                    y_prob_ref = RF.predict_proba(X_REF)[:, 1]\n",
    "                    os.makedirs(os.path.join(PATH_TO_CORRELATIONS, \"M\"), exist_ok=True)\n",
    "                    filename = f'{name}_ref_probs.npz' \n",
    "                    np.savez_compressed(os.path.join(PATH_TO_CORRELATIONS, \"M\", filename), y_prob_ref=y_prob_ref)\n",
    "\n",
    "            else:\n",
    "                print(f\"Too few positive compounds for {activity_type}_{unit}_ql .. {strain} ... ({positives})\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Please revise. df_quant and df_qual are empty...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1936e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_LM = pd.DataFrame(MERGED_LM, columns=[\"name\", \"activity_type\", \"unit\", \"expert_cutoff\", \"direction\", \"assay_type\", \"target_type_curated_extra\", \"bao_label\", \"strain\", \n",
    "                                      \"target_chembl_id\", \"n_assays\", \"n_cpds_union\", \"positives\", \"ratio\", \"average_auroc\", \"stds\", \"assay_keys\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9406af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_LM.to_csv(os.path.join(OUTPUT, pathogen_code, 'merged_LM.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca66474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28337, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accepted_compounds), len(set(MERGED_COMPOUNDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928f8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
