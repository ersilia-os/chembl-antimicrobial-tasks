{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc7f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".dataframe td, .dataframe th {\n",
       "    white-space: nowrap !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import gzip\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td, .dataframe th {\n",
    "    white-space: nowrap !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490fd441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step XX\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "# root = os.path.dirname(os.path.abspath(__file__))\n",
    "root = \".\"\n",
    "sys.path.append(os.path.join(root, \"..\", \"src\"))\n",
    "from default import DATAPATH, CONFIGPATH\n",
    "\n",
    "# Load pathogen info\n",
    "# pathogen_code = sys.argv[1]\n",
    "pathogen_code = 'mtuberculosis'\n",
    "df = pd.read_csv(os.path.join(CONFIGPATH, 'pathogens.csv'))\n",
    "row = df.loc[df[\"code\"].eq(pathogen_code)]\n",
    "if row.empty: \n",
    "    raise SystemExit(f\"Unknown code: {pathogen_code}\")\n",
    "pathogen = row.iloc[0][\"pathogen\"]\n",
    "\n",
    "print(\"Step XX\")\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT = os.path.join(root, \"..\", \"output\")\n",
    "\n",
    "# Shared columns\n",
    "KEYS = [\"assay_id\", \"activity_type\", \"unit\"]\n",
    "\n",
    "# Columns to take from datasets table\n",
    "COLUMNS_DATASETS = [\"equal\", 'higher', 'lower', \"target_type_curated_extra\", \"dataset_type\", \"cpds_qt\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4612192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_gz_csvs_from_zip(zip_path):\n",
    "    dfs = {}\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for name in z.namelist():\n",
    "            if name.endswith(\".csv.gz\"):\n",
    "                with z.open(name) as f:\n",
    "                    dfs[name] = pd.read_csv(f, compression=\"gzip\")\n",
    "    return dfs\n",
    "\n",
    "def get_all_results_from_individual_modeling(INDIVIDUAL_LM, LABELS):\n",
    "    RESULTS, CONSIDERED_ASSAYS = {}, {}\n",
    "    for LABEL in LABELS:\n",
    "        RESULTS[LABEL] = {}\n",
    "        CONSIDERED_ASSAYS[LABEL] = set()\n",
    "        rows = INDIVIDUAL_LM[INDIVIDUAL_LM[LABEL]][[\"assay_id\", \"activity_type\", \"unit\", \"expert_cutoff\", f\"{LABEL}_AVG\"]].values\n",
    "        for assay_id, activity_type, unit, expert_cutoff, auroc in rows:\n",
    "            key = (assay_id, activity_type, unit)\n",
    "            CONSIDERED_ASSAYS[LABEL].add(key)\n",
    "            if auroc > 0.7:\n",
    "                if key not in RESULTS[LABEL]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "                elif auroc > RESULTS[LABEL][key][1]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "    return RESULTS, CONSIDERED_ASSAYS\n",
    "\n",
    "def where_considered(key, LABELS, CONSIDERED_ASSAYS):\n",
    "    considered = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in CONSIDERED_ASSAYS[LABEL]:\n",
    "            considered.append(LABEL)\n",
    "    if len(considered) > 0:\n",
    "        return \";\".join(considered)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def where_accepted(key, LABELS, ACCEPTED_ASSAYS):\n",
    "    accepted = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in ACCEPTED_ASSAYS[LABEL]:\n",
    "            accepted.append(LABEL)\n",
    "    if len(accepted) > 0:\n",
    "        return \";\".join(accepted)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_filtered_assay_master(assay_df, activity_type, unit, target_type_curated_extra, bao_label, strain):\n",
    "    if type(unit) == str:\n",
    "        df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                    (assay_df['unit'] == unit) &\n",
    "                    (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (assay_df['bao_label'] == bao_label) &\n",
    "                    (assay_df['strain'] == strain)]\n",
    "    else:\n",
    "        df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                    (assay_df['unit'].isna()) &\n",
    "                    (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (assay_df['bao_label'] == bao_label) &\n",
    "                    (assay_df['strain'] == strain)]\n",
    "    return df\n",
    "\n",
    "def load_expert_cutoffs(CONFIGPATH):\n",
    "    \"\"\"\n",
    "    Load expert cutoffs from the manual curation CSV and return them as a dictionary.\n",
    "\n",
    "    The CSV is expected at:\n",
    "        {CONFIGPATH}/manual_curation/expert_cutoffs.csv\n",
    "\n",
    "    The returned dictionary maps:\n",
    "        (activity_type, unit, target_type, pathogen_code) -> expert_cutoff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    CONFIGPATH : str\n",
    "        Path to the config folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of expert cutoffs keyed by\n",
    "        (activity_type, unit, target_type, pathogen_code).\n",
    "    \"\"\"\n",
    "    # Load expert cut-offs\n",
    "    EXPERT_CUTOFFS = pd.read_csv(os.path.join(CONFIGPATH, \"expert_cutoffs.csv\"))\n",
    "\n",
    "    EXPERT_CUTOFFS = {\n",
    "        (a, b, c, d): [float(k) for k in e.split(\";\")]\n",
    "        for a, b, c, d, e in EXPERT_CUTOFFS[\n",
    "            [\"activity_type\", \"unit\", \"target_type\", \"pathogen_code\", \"expert_cutoff\"]\n",
    "        ].values\n",
    "    }\n",
    "\n",
    "    return EXPERT_CUTOFFS\n",
    "\n",
    "def load_ecfp_all(h5_path):\n",
    "    \"\"\"Load all ECFP (Morgan count) fingerprints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing datasets \"SMILES\" and \"X_morgan\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, np.ndarray]\n",
    "        Mapping {chembl_id: fingerprint (np.int8, shape (nBits,))}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        meta = f[\"SMILES\"][:, 3].astype(str)\n",
    "        fps  = f[\"X_morgan\"][:]  # Load ALL\n",
    "\n",
    "    return {cid: fp for cid, fp in zip(meta, fps)}\n",
    "\n",
    "def KFoldTrain(X, Y, n_splits=4, n_estimators=100, random_state=42):\n",
    "    \"\"\"Stratified K-fold training/eval with RandomForest; returns mean AUROC and std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Binary labels (n_samples,).\n",
    "    n_splits : int\n",
    "        Number of folds.\n",
    "    n_estimators : int\n",
    "        Number of trees in the random forest.\n",
    "    random_state : int\n",
    "        RNG seed (also used for fold shuffling).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        (mean_auroc, std_auroc) rounded to 3 decimals.\n",
    "    \"\"\"\n",
    "    def init_RF():\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aurocs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = init_RF()\n",
    "        rf.fit(X_train, Y_train)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "        aurocs.append(roc_auc_score(Y_test, y_prob))\n",
    "\n",
    "    return round(float(np.mean(aurocs)), 3), round(float(np.std(aurocs)), 3)\n",
    "\n",
    "def TrainRF(X, Y, n_estimators=100):\n",
    "    \"\"\"Train a RandomForestClassifier on all provided data and return the fitted model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Labels (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RandomForestClassifier\n",
    "        Fitted classifier.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    rf.fit(X, Y)\n",
    "    return rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3110ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging assay metadata\n",
      "Mapping assays to compounds\n",
      "Loading individual datasets\n",
      "Loaded quantitative: 26370 datasets\n",
      "Loaded qualitative: 1536 datasets\n"
     ]
    }
   ],
   "source": [
    "# Load assays info\n",
    "print(\"Merging assay metadata\")\n",
    "ASSAYS_CLEANED = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_cleaned.csv\"))\n",
    "ASSAYS_PARAMETERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_parameters.csv\"))\n",
    "ASSAYS_DATASETS_ = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_datasets.csv\"))\n",
    "INDIVIDUAL_LM = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"individual_LM.csv\"))\n",
    "\n",
    "# Get assay to quantitative data info\n",
    "assay_to_qt_info = defaultdict(list)\n",
    "for assay_id, activity_type, unit, expert_cutoff, ratio_qt in ASSAYS_DATASETS_[['assay_id', 'activity_type', 'unit', 'expert_cutoff', 'ratio_qt']].values:\n",
    "    assay_to_qt_info[tuple([assay_id, activity_type, unit])].append([expert_cutoff, ratio_qt])\n",
    "\n",
    "# Unique row per assay\n",
    "ASSAYS_DATASETS = ASSAYS_DATASETS_[KEYS + COLUMNS_DATASETS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Get cutoffs and ratios\n",
    "cutoffs = [\";\".join([str(j[0]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "ratios = [\";\".join([str(j[1]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "cutoffs = [i if i != 'nan' else np.nan for i in cutoffs]\n",
    "ratios = [i if i != 'nan' else np.nan for i in ratios]\n",
    "\n",
    "# Store results\n",
    "ASSAYS_DATASETS.insert(8, 'cutoffs', cutoffs)\n",
    "ASSAYS_DATASETS.insert(9, 'ratios', ratios)\n",
    "\n",
    "# Merge everything\n",
    "ASSAYS_MASTER = ASSAYS_CLEANED.merge(ASSAYS_PARAMETERS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "ASSAYS_MASTER = ASSAYS_MASTER.merge(ASSAYS_DATASETS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Dict mapping assay_id, activity_type and unit to a set of compound ChEMBL IDs\n",
    "print(\"Mapping assays to compounds\")\n",
    "ChEMBL = pd.read_csv(os.path.join(OUTPUT, pathogen_code, f\"{pathogen_code}_ChEMBL_cleaned_data.csv.gz\"), low_memory=False)\n",
    "ASSAY_TO_COMPOUNDS = defaultdict(set)\n",
    "for assay_id, activity_type, unit, compound_chembl_id in ChEMBL[[\"assay_chembl_id\", \"activity_type\", \"unit\", \"compound_chembl_id\"]].values:\n",
    "    ASSAY_TO_COMPOUNDS[(assay_id, activity_type, unit)].add(compound_chembl_id)\n",
    "del ChEMBL\n",
    "\n",
    "# Loading quantitative and qualitative datasets\n",
    "print(\"Loading individual datasets\")\n",
    "qt_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_qt.zip\")\n",
    "ql_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_ql.zip\")\n",
    "dfs_qt = load_all_gz_csvs_from_zip(qt_zip)\n",
    "dfs_ql = load_all_gz_csvs_from_zip(ql_zip)\n",
    "print(\"Loaded quantitative:\", len(dfs_qt), \"datasets\")\n",
    "print(\"Loaded qualitative:\", len(dfs_ql), \"datasets\")\n",
    "\n",
    "# Get results from individual modeling ABCD\n",
    "LABELS = ['A', 'B', 'C', 'D']\n",
    "ACCEPTED_ASSAYS, CONSIDERED_ASSAYS = get_all_results_from_individual_modeling(INDIVIDUAL_LM, LABELS)\n",
    "\n",
    "col_accepted, col_considered = [], []\n",
    "for assay_id, activity_type, unit in ASSAYS_MASTER[[\"assay_id\", \"activity_type\", \"unit\"]].values:\n",
    "    # Get strategies in which this assay is considered and accepted\n",
    "    key = tuple([assay_id, activity_type, unit])\n",
    "    col_considered.append(where_considered(key, LABELS, CONSIDERED_ASSAYS))\n",
    "    col_accepted.append(where_accepted(key, LABELS, ACCEPTED_ASSAYS))\n",
    "ASSAYS_MASTER['Accepted'] = col_accepted\n",
    "ASSAYS_MASTER['Considered'] = col_considered\n",
    "\n",
    "# Reorder columns\n",
    "ALL_COLS = [\"assay_id\", \"assay_type\", \"assay_organism\", \"target_organism\", \"organism_curated\", \"doc_chembl_id\", \"target_type\", \"target_type_curated\", \"target_type_curated_extra\", \n",
    "          \"target_chembl_id\", \"target_chembl_id_curated\", \"target_name_curated\", \"bao_label\", \"source_label\", \"strain\", \"atcc_id\", \"mutations\", \"known_drug_resistances\", \"media\",\n",
    "          \"activity_type\", \"unit\", \"activities\", \"nan_values\", \"cpds\", \"frac_cs\", \"direction\", \"act_flag\", 'inact_flag', \"equal\", \"higher\", \"lower\", \"dataset_type\", \"cutoffs\", \"ratios\", \n",
    "          \"cpds_qt\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", 'Accepted', 'Considered']\n",
    "ASSAYS_MASTER = ASSAYS_MASTER[ALL_COLS]\n",
    "\n",
    "# Get accepted assays and accepted compounds in ABCD\n",
    "accepted_assays = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna() == False)][['assay_id', 'activity_type', 'unit']].values\n",
    "accepted_compounds = set([j for i in accepted_assays for j in ASSAY_TO_COMPOUNDS[tuple(i)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4588e00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying potential assays to merge\n",
      "Organisms...\n",
      "Single proteins...\n",
      "Loading ECFPs...\n"
     ]
    }
   ],
   "source": [
    "# Filtering assays\n",
    "print(\"Identifying potential assays to merge\")\n",
    "print(\"Organisms...\")\n",
    "keys_organism = [\"activity_type\", \"unit\", \"target_type_curated_extra\", \"bao_label\", \"strain\"]\n",
    "FILTERED_ASSAYS_ORGANISM = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna()) & (ASSAYS_MASTER['target_type_curated_extra'] == 'ORGANISM')].copy()\n",
    "TO_MERGE_ORGANISM = (FILTERED_ASSAYS_ORGANISM\n",
    "                    .groupby(keys_organism, dropna=False)\n",
    "                    .agg(n_cpds_red=(\"cpds\", \"sum\"), n_assays=(\"assay_id\", \"size\"))\n",
    "                    .reset_index()\n",
    "                    .sort_values(\"n_cpds_red\", ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Single proteins...\")\n",
    "keys_single_protein = [\"activity_type\", \"unit\", \"target_type_curated_extra\", \"bao_label\", \"strain\", 'target_chembl_id']  # target name\n",
    "FILTERED_ASSAYS_SINGLE_PROTEIN = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna()) & (ASSAYS_MASTER['target_type_curated_extra'] == 'SINGLE PROTEIN')].copy()\n",
    "TO_MERGE_SINGLE_PROTEIN = (FILTERED_ASSAYS_SINGLE_PROTEIN\n",
    "                    .groupby(keys_single_protein, dropna=False)\n",
    "                    .agg(n_cpds_red=(\"cpds\", \"sum\"), n_assays=(\"assay_id\", \"size\"))\n",
    "                    .reset_index()\n",
    "                    .sort_values(\"n_cpds_red\", ascending=False))\n",
    "\n",
    "# # Get cumulative proportions\n",
    "# TO_MERGE_ORGANISM[\"cum_prop\"] = TO_MERGE_ORGANISM[\"n_cpds_red\"].cumsum() / TO_MERGE_ORGANISM[\"n_cpds_red\"].sum()\n",
    "# TO_MERGE_SINGLE_PROTEIN[\"cum_prop\"] = TO_MERGE_SINGLE_PROTEIN[\"n_cpds_red\"].cumsum() / TO_MERGE_SINGLE_PROTEIN[\"n_cpds_red\"].sum()\n",
    "\n",
    "# Filtering only activity type - unit pairs relevant for merging\n",
    "TO_MERGE_ORGANISM = TO_MERGE_ORGANISM[(TO_MERGE_ORGANISM['n_cpds_red'] > 1000) & (TO_MERGE_ORGANISM['n_assays'] > 1)]\n",
    "TO_MERGE_SINGLE_PROTEIN = TO_MERGE_SINGLE_PROTEIN[(TO_MERGE_SINGLE_PROTEIN['n_cpds_red'] > 1000) & (TO_MERGE_SINGLE_PROTEIN['n_assays'] > 1)]\n",
    "\n",
    "# Load expert cut-offs\n",
    "EXPERT_CUTOFFS = load_expert_cutoffs(CONFIGPATH)\n",
    "\n",
    "# Loading Morgan fingerprints\n",
    "print(\"Loading ECFPs...\")\n",
    "PATH_TO_ECFPs = os.path.join(DATAPATH, \"chembl_processed\", \"ChEMBL_ECFPs.h5\")\n",
    "ecfps = load_ecfp_all(PATH_TO_ECFPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94231d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_SET !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a818b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_compounds = []\n",
    "\n",
    "for merging in cpds_grouped.itertuples():\n",
    "\n",
    "    # Get data\n",
    "    activity_type = merging.activity_type\n",
    "    unit = merging.unit\n",
    "    target_type_curated_extra = merging.target_type_curated_extra\n",
    "    bao_label = merging.bao_label\n",
    "    strain = merging.strain\n",
    "\n",
    "    # Filter master table\n",
    "    df = get_filtered_assay_master(activity_type, unit, target_type_curated_extra, bao_label, strain)\n",
    "\n",
    "    # Get quantitative and qualitative\n",
    "    df_quant = df[(df['dataset_type'] == 'quantitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "    df_qual = df[(df['dataset_type'] == 'qualitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "\n",
    "    # Quantitative\n",
    "    if len(df_quant) > 0 and sum(df_quant['cpds']) > 1000:\n",
    "\n",
    "        # For each expert cut-off\n",
    "        for expert_cutoff in EXPERT_CUTOFFS[(activity_type, unit, target_type_curated_extra, pathogen_code)]:\n",
    "            print(expert_cutoff)\n",
    "            assays = df_quant['assay_id'].tolist()\n",
    "            files = [f\"{i}_{activity_type}_{unit}_qt_{expert_cutoff}.csv.gz\" for i in assays]\n",
    "            data = [dfs_qt[f].assign(assay_id=a) for a, f in zip(assays, files)]\n",
    "            data = pd.concat(data, ignore_index=True)\n",
    "            data = data.sort_values(\"value\", ascending=True).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "            if len(data) > 1000:\n",
    "                X = np.array(data['compound_chembl_id'].map(ecfps).to_list())\n",
    "                Y = np.array(data['bin'].tolist())\n",
    "                if sum(Y) > 50 and sum(Y) / len(Y):\n",
    "                    print(f\"Merging ... Activity type: {activity_type}, Unit: {unit}, Cutoff: {expert_cutoff}\")\n",
    "                    print(f\"\\tCompounds: {len(X)}\", f\"Positives: {sum(Y)} ({round(100 * sum(Y) / len(Y), 1)}%)\")\n",
    "                    # 4Fold Cros Validation\n",
    "                    average_auroc, stds = KFoldTrain(X, Y, n_splits=5, n_estimators=100)\n",
    "                    print(f\"\\tMean AUROC: {average_auroc} Â± {stds}\")\n",
    "                    if average_auroc > 0.7:\n",
    "                        merged_compounds.extend(data['compound_chembl_id'].tolist())\n",
    "\n",
    "                    # # If performance is good enough, train on full data and predict on reference set\n",
    "                    # if average_auroc > 0.7:\n",
    "                    #     RF = TrainRF(X, Y, n_estimators=100)\n",
    "                    #     y_prob_ref = RF.predict_proba(X_REF)[:, 1]\n",
    "                    #     os.makedirs(os.path.join(PATH_TO_CORRELATIONS, LABEL), exist_ok=True)\n",
    "                    #     np.savez_compressed(os.path.join(PATH_TO_CORRELATIONS, LABEL, filename.replace(\".csv.gz\", \"_ref_probs.npz\")), y_prob_ref=y_prob_ref)\n",
    "\n",
    "\n",
    "    if len(df_qual) > 0 and sum(df_qual['cpds']) > 1000:\n",
    "\n",
    "        # Get assays, files and data\n",
    "        assays = df_qual['assay_id'].tolist()\n",
    "        files = [f\"{i}_{activity_type}_{unit}_ql.csv.gz\" for i in assays]\n",
    "        data = [dfs_ql[file] for file in files]\n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        data = data.sort_values(\"bin\").drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "        if len(data) > 1000:\n",
    "            ...\n",
    "        else:\n",
    "            print(f\"Too few data for {activity_type}, {unit}, {target_type_curated_extra}, {bao_label}, {strain}... ({sum(df['cpds'])} --> {len(data)} compounds) after merging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299851f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_compounds = set(merged_compounds)\n",
    "len(merged_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf77c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in merged_compounds if i not in accepted_compounds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882331ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_compounds.union(accepted_compounds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
