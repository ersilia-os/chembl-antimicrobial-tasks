{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05f0b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".dataframe td, .dataframe th {\n",
       "    white-space: nowrap !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import gzip\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td, .dataframe th {\n",
    "    white-space: nowrap !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5afb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "# root = os.path.dirname(os.path.abspath(__file__))\n",
    "root = \".\"\n",
    "sys.path.append(os.path.join(root, \"..\", \"src\"))\n",
    "from default import DATAPATH, CONFIGPATH\n",
    "\n",
    "# Load pathogen info\n",
    "# pathogen_code = sys.argv[1]\n",
    "pathogen_code = 'mtuberculosis'\n",
    "df = pd.read_csv(os.path.join(CONFIGPATH, 'pathogens.csv'))\n",
    "row = df.loc[df[\"code\"].eq(pathogen_code)]\n",
    "if row.empty: \n",
    "    raise SystemExit(f\"Unknown code: {pathogen_code}\")\n",
    "pathogen = row.iloc[0][\"pathogen\"]\n",
    "\n",
    "print(\"Step 15\")\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT = os.path.join(root, \"..\", \"output\")\n",
    "\n",
    "# Shared columns\n",
    "KEYS = [\"assay_id\", \"activity_type\", \"unit\"]\n",
    "\n",
    "# Columns to take from datasets and clusters tables\n",
    "COLUMNS_DATASETS = [\"equal\", 'higher', 'lower', \"target_type_curated_extra\", \"dataset_type\", \"cpds_qt\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\"]\n",
    "COLUMNS_CLUSTERS = ['clusters_0.3', 'clusters_0.6', 'clusters_0.85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c937f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_gz_csvs_from_zip(zip_path):\n",
    "    \"\"\"Read all ``*.csv.gz`` members from a ZIP into DataFrames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zip_path : str | pathlib.Path\n",
    "        Path to the ZIP archive.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, pandas.DataFrame]\n",
    "        Mapping of ZIP member name -> loaded DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for name in z.namelist():\n",
    "            if name.endswith(\".csv.gz\"):\n",
    "                with z.open(name) as f:\n",
    "                    dfs[name] = pd.read_csv(f, compression=\"gzip\")\n",
    "    return dfs\n",
    "\n",
    "def get_all_results_from_individual_modeling(INDIVIDUAL_LM, LABELS=['A', 'B', 'C', 'D']):\n",
    "    \"\"\"Collect best AUROC (>0.7) per (assay_id, activity_type, unit) for each label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RESULTS : dict[str, dict[tuple, list]]\n",
    "        Per label: (assay_id, activity_type, unit) -> [expert_cutoff, best_auroc],\n",
    "        considering only rows with AUROC > 0.7.\n",
    "    CONSIDERED_ASSAYS : dict[str, set[tuple]]\n",
    "        Per label: set of all (assay_id, activity_type, unit) keys encountered\n",
    "        (no AUROC threshold applied).\n",
    "    \"\"\"\n",
    "    RESULTS, CONSIDERED_ASSAYS = {}, {}\n",
    "    for LABEL in LABELS:\n",
    "        RESULTS[LABEL] = {}\n",
    "        CONSIDERED_ASSAYS[LABEL] = set()\n",
    "        rows = INDIVIDUAL_LM[INDIVIDUAL_LM[LABEL]][[\"assay_id\", \"activity_type\", \"unit\", \"expert_cutoff\", f\"{LABEL}_AVG\"]].values\n",
    "        for assay_id, activity_type, unit, expert_cutoff, auroc in rows:\n",
    "            key = (assay_id, activity_type, unit)\n",
    "            CONSIDERED_ASSAYS[LABEL].add(key)\n",
    "            if auroc > 0.7:\n",
    "                if key not in RESULTS[LABEL]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "                elif auroc > RESULTS[LABEL][key][1]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "    return RESULTS, CONSIDERED_ASSAYS\n",
    "\n",
    "def where_considered(key, LABELS, CONSIDERED_ASSAYS):\n",
    "    \"\"\"Return labels (semicolon-separated) where `key` was considered; else NaN.\"\"\"\n",
    "    considered = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in CONSIDERED_ASSAYS[LABEL]:\n",
    "            considered.append(LABEL)\n",
    "    if len(considered) > 0:\n",
    "        return \";\".join(considered)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def where_accepted(key, LABELS, ACCEPTED_ASSAYS):\n",
    "    \"\"\"Return labels (semicolon-separated) where `key` was accepted; else NaN.\"\"\"\n",
    "    accepted = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in ACCEPTED_ASSAYS[LABEL]:\n",
    "            accepted.append(LABEL)\n",
    "    if len(accepted) > 0:\n",
    "        return \";\".join(accepted)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def get_filtered_assay_master(assay_df, activity_type, unit, target_type_curated_extra, bao_label, strain):\n",
    "    \"\"\"Filter `assay_df` by metadata fields, treating non-string `unit` as missing (NaN).\"\"\"\n",
    "    if type(unit) == str:\n",
    "        df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                    (assay_df['unit'] == unit) &\n",
    "                    (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (assay_df['bao_label'] == bao_label) &\n",
    "                    (assay_df['strain'] == strain)]\n",
    "    else:\n",
    "        df = assay_df[(assay_df['activity_type'] == activity_type) & \n",
    "                    (assay_df['unit'].isna()) &\n",
    "                    (assay_df['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (assay_df['bao_label'] == bao_label) &\n",
    "                    (assay_df['strain'] == strain)]\n",
    "    return df\n",
    "\n",
    "def load_expert_cutoffs(CONFIGPATH):\n",
    "    \"\"\"\n",
    "    Load expert cutoffs from the manual curation CSV and return them as a dictionary.\n",
    "\n",
    "    The CSV is expected at:\n",
    "        {CONFIGPATH}/manual_curation/expert_cutoffs.csv\n",
    "\n",
    "    The returned dictionary maps:\n",
    "        (activity_type, unit, target_type, pathogen_code) -> expert_cutoff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    CONFIGPATH : str\n",
    "        Path to the config folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of expert cutoffs keyed by\n",
    "        (activity_type, unit, target_type, pathogen_code).\n",
    "    \"\"\"\n",
    "    # Load expert cut-offs\n",
    "    EXPERT_CUTOFFS = pd.read_csv(os.path.join(CONFIGPATH, \"expert_cutoffs.csv\"))\n",
    "\n",
    "    EXPERT_CUTOFFS = {\n",
    "        (a, b, c, d): [float(k) for k in e.split(\";\")]\n",
    "        for a, b, c, d, e in EXPERT_CUTOFFS[\n",
    "            [\"activity_type\", \"unit\", \"target_type\", \"pathogen_code\", \"expert_cutoff\"]\n",
    "        ].values\n",
    "    }\n",
    "\n",
    "    return EXPERT_CUTOFFS\n",
    "\n",
    "def load_ecfp_all(h5_path):\n",
    "    \"\"\"Load all ECFP (Morgan count) fingerprints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing datasets \"SMILES\" and \"X_morgan\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, np.ndarray]\n",
    "        Mapping {chembl_id: fingerprint (np.int8, shape (nBits,))}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        meta = f[\"SMILES\"][:, 3].astype(str)\n",
    "        fps  = f[\"X_morgan\"][:]  # Load ALL\n",
    "\n",
    "    return {cid: fp for cid, fp in zip(meta, fps)}\n",
    "\n",
    "def KFoldTrain(X, Y, n_splits=4, n_estimators=100, random_state=42):\n",
    "    \"\"\"Stratified K-fold training/eval with RandomForest; returns mean AUROC and std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Binary labels (n_samples,).\n",
    "    n_splits : int\n",
    "        Number of folds.\n",
    "    n_estimators : int\n",
    "        Number of trees in the random forest.\n",
    "    random_state : int\n",
    "        RNG seed (also used for fold shuffling).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        (mean_auroc, std_auroc) rounded to 3 decimals.\n",
    "    \"\"\"\n",
    "    def init_RF():\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aurocs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = init_RF()\n",
    "        rf.fit(X_train, Y_train)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "        aurocs.append(roc_auc_score(Y_test, y_prob))\n",
    "\n",
    "    return round(float(np.mean(aurocs)), 3), round(float(np.std(aurocs)), 3)\n",
    "\n",
    "def TrainRF(X, Y, n_estimators=100):\n",
    "    \"\"\"Train a RandomForestClassifier on all provided data and return the fitted model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Labels (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RandomForestClassifier\n",
    "        Fitted classifier.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    rf.fit(X, Y)\n",
    "    return rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7098d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading assay data\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'ratio_qt_x', 'expert_cutoff_x', 'pos_qt_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[LABEL]]\n\u001b[1;32m     43\u001b[0m COLS \u001b[38;5;241m=\u001b[39m KEYS \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpert_cutoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_qt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mratio_qt\u001b[39m\u001b[38;5;124m\"\u001b[39m, LABEL, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLABEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_AVG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m ASSAYS_MASTER \u001b[38;5;241m=\u001b[39m \u001b[43mASSAYS_MASTER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCOLS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKEYS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m ASSAYS_MASTER[LABEL] \u001b[38;5;241m=\u001b[39m (ASSAYS_MASTER[LABEL]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;28;01mFalse\u001b[39;00m)) \n",
      "File \u001b[0;32m~/miniconda3/envs/camt/lib/python3.10/site-packages/pandas/core/frame.py:10819\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10800\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10801\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10815\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10817\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10828\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/camt/lib/python3.10/site-packages/pandas/core/reshape/merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    171\u001b[0m         left_df,\n\u001b[1;32m    172\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/camt/lib/python3.10/site-packages/pandas/core/reshape/merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m~/miniconda3/envs/camt/lib/python3.10/site-packages/pandas/core/reshape/merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[1;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[0;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    849\u001b[0m         join_index,\n\u001b[1;32m    850\u001b[0m         left_indexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/camt/lib/python3.10/site-packages/pandas/core/reshape/merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[0;34m(left, right, suffixes)\u001b[0m\n\u001b[1;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[0;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2760\u001b[0m     )\n\u001b[1;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[0;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'ratio_qt_x', 'expert_cutoff_x', 'pos_qt_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "# Load assays info\n",
    "print(\"Loading assay data\")\n",
    "ASSAYS_CLEANED = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_cleaned.csv\"))\n",
    "ASSAYS_CLUSTERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_clusters.csv\"))\n",
    "ASSAYS_PARAMETERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_parameters.csv\"))\n",
    "ASSAYS_DATASETS_ = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_datasets.csv\"))\n",
    "INDIVIDUAL_LM = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"individual_LM.csv\"))\n",
    "MERGED_LM = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"merged_LM.csv\"))\n",
    "\n",
    "# Getting keys\n",
    "KEYS = [\"assay_id\", \"activity_type\", \"unit\"]\n",
    "\n",
    "# Get assay to quantitative data info to collapse ASSAY_DATASETS_ (1 row per assay)\n",
    "assay_to_qt_info = defaultdict(list)\n",
    "for assay_id, activity_type, unit, expert_cutoff, ratio_qt in ASSAYS_DATASETS_[['assay_id', 'activity_type', 'unit', 'expert_cutoff', 'ratio_qt']].values:\n",
    "    assay_to_qt_info[tuple([assay_id, activity_type, unit])].append([expert_cutoff, ratio_qt])\n",
    "\n",
    "# Unique row per assay\n",
    "ASSAYS_DATASETS = ASSAYS_DATASETS_[KEYS + COLUMNS_DATASETS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Get cutoffs and ratios\n",
    "cutoffs = [\";\".join([str(j[0]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "ratios = [\";\".join([str(j[1]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "cutoffs = [i if i != 'nan' else np.nan for i in cutoffs]\n",
    "ratios = [i if i != 'nan' else np.nan for i in ratios]\n",
    "\n",
    "# Store results\n",
    "ASSAYS_DATASETS.insert(8, 'cutoffs', cutoffs)\n",
    "ASSAYS_DATASETS.insert(9, 'ratios', ratios)\n",
    "\n",
    "# Select particular columns from clusters\n",
    "ASSAYS_CLUSTERS = ASSAYS_CLUSTERS[KEYS + COLUMNS_CLUSTERS]\n",
    "\n",
    "# Merge everything\n",
    "ASSAYS_MASTER = ASSAYS_CLEANED.merge(ASSAYS_PARAMETERS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "ASSAYS_MASTER = ASSAYS_MASTER.merge(ASSAYS_CLUSTERS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "ASSAYS_MASTER = ASSAYS_MASTER.merge(ASSAYS_DATASETS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Create light model columns\n",
    "for LABEL in \"ABCD\":\n",
    "    df = INDIVIDUAL_LM.sort_values(f'{LABEL}_AVG', ascending=False).drop_duplicates(subset=KEYS, keep='first').reset_index(drop=True)\n",
    "    df = df[df[LABEL]]\n",
    "    COLS = KEYS + [\"expert_cutoff\", \"pos_qt\", \"ratio_qt\", LABEL, f\"{LABEL}_AVG\"]\n",
    "    ASSAYS_MASTER = ASSAYS_MASTER.merge(df[COLS], on=KEYS, how='left', validate=\"1:1\")\n",
    "    ASSAYS_MASTER[LABEL] = (ASSAYS_MASTER[LABEL].astype(\"boolean\").fillna(False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd663a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ASSAYS_MASTER[ASSAYS_MASTER['A_AVG'] > 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc82406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading quantitative and qualitative datasets\n",
    "print(\"Loading individual datasets\")\n",
    "qt_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_qt.zip\")\n",
    "ql_zip = os.path.join(OUTPUT, pathogen_code, \"datasets\", \"datasets_ql.zip\")\n",
    "dfs_qt = load_all_gz_csvs_from_zip(qt_zip)\n",
    "dfs_ql = load_all_gz_csvs_from_zip(ql_zip)\n",
    "print(\"Loaded quantitative:\", len(dfs_qt), \"datasets\")\n",
    "print(\"Loaded qualitative:\", len(dfs_ql), \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assays considered at some point\n",
    "considered_A = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['A']][KEYS].values])\n",
    "considered_B = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['B']][KEYS].values])\n",
    "considered_C = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['C']][KEYS].values])\n",
    "considered_D = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['D']][KEYS].values])\n",
    "\n",
    "# Assays accepted at some point\n",
    "accepted_A = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['A_AVG'] > 0.7][KEYS].values])\n",
    "accepted_B = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['B_AVG'] > 0.7][KEYS].values])\n",
    "accepted_C = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['C_AVG'] > 0.7][KEYS].values])\n",
    "accepted_D = set([tuple(i) for i in INDIVIDUAL_LM[INDIVIDUAL_LM['D_AVG'] > 0.7][KEYS].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(accepted_A))\n",
    "print(len(accepted_B))\n",
    "print(len(accepted_C))\n",
    "print(len(accepted_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38100dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIVIDUAL_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b519ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
