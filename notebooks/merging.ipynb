{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc7f918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".dataframe td, .dataframe th {\n",
       "    white-space: nowrap !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import spearmanr\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import random\n",
    "import gzip\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td, .dataframe th {\n",
    "    white-space: nowrap !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba3b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490fd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "# root = os.path.dirname(os.path.abspath(__file__))\n",
    "root = \".\"\n",
    "sys.path.append(os.path.join(root, \"..\", \"src\"))\n",
    "from default import DATAPATH, CONFIGPATH\n",
    "\n",
    "# Load pathogen info\n",
    "# pathogen_code = sys.argv[1]\n",
    "pathogen_code = 'mtuberculosis'\n",
    "df = pd.read_csv(os.path.join(CONFIGPATH, 'pathogens.csv'))\n",
    "row = df.loc[df[\"code\"].eq(pathogen_code)]\n",
    "if row.empty: \n",
    "    raise SystemExit(f\"Unknown code: {pathogen_code}\")\n",
    "pathogen = row.iloc[0][\"pathogen\"]\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT = os.path.join(root, \"..\", \"output\")\n",
    "\n",
    "# Shared columns\n",
    "KEYS = [\"assay_id\", \"activity_type\", \"unit\"]\n",
    "\n",
    "# Columns to take from datasets table\n",
    "COLUMNS_DATASETS = [\"equal\", 'higher', 'lower', \"target_type_curated_extra\", \"dataset_type\", \"cpds_qt\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3110ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load assays info\n",
    "ASSAYS_CLEANED = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_cleaned.csv\"))\n",
    "# ASSAYS_CLUSTERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_clusters.csv\"))\n",
    "ASSAYS_PARAMETERS = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_parameters.csv\"))\n",
    "ASSAYS_DATASETS_ = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"assays_datasets.csv\"))\n",
    "INDIVIDUAL_LM = pd.read_csv(os.path.join(OUTPUT, pathogen_code, \"individual_LM.csv\"))\n",
    "\n",
    "# Get assay to quantitative data info\n",
    "assay_to_qt_info = defaultdict(list)\n",
    "for assay_id, activity_type, unit, expert_cutoff, ratio_qt in ASSAYS_DATASETS_[['assay_id', 'activity_type', 'unit', 'expert_cutoff', 'ratio_qt']].values:\n",
    "    assay_to_qt_info[tuple([assay_id, activity_type, unit])].append([expert_cutoff, ratio_qt])\n",
    "\n",
    "# Unique row per assay\n",
    "ASSAYS_DATASETS = ASSAYS_DATASETS_[KEYS + COLUMNS_DATASETS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Get cutoffs and ratios\n",
    "cutoffs = [\";\".join([str(j[0]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "ratios = [\";\".join([str(j[1]) for j in assay_to_qt_info[tuple(i)]]) for i in ASSAYS_DATASETS[['assay_id', 'activity_type', 'unit']].values]\n",
    "cutoffs = [i if i != 'nan' else np.nan for i in cutoffs]\n",
    "ratios = [i if i != 'nan' else np.nan for i in ratios]\n",
    "\n",
    "# Store results\n",
    "ASSAYS_DATASETS.insert(8, 'cutoffs', cutoffs)\n",
    "ASSAYS_DATASETS.insert(9, 'ratios', ratios)\n",
    "\n",
    "# Merge everything\n",
    "ASSAYS_MASTER = ASSAYS_CLEANED.merge(ASSAYS_PARAMETERS,on=KEYS, how=\"left\", validate=\"1:1\")\n",
    "ASSAYS_MASTER = ASSAYS_MASTER.merge(ASSAYS_DATASETS,on=KEYS, how=\"left\", validate=\"1:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b1306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict mapping assay_id, activity_type and unit to a set of compound ChEMBL IDs\n",
    "ChEMBL = pd.read_csv(os.path.join(OUTPUT, pathogen_code, f\"{pathogen_code}_ChEMBL_cleaned_data.csv.gz\"), low_memory=False)\n",
    "ASSAY_TO_COMPOUNDS = defaultdict(set)\n",
    "for assay_id, activity_type, unit, compound_chembl_id in ChEMBL[[\"assay_chembl_id\", \"activity_type\", \"unit\", \"compound_chembl_id\"]].values:\n",
    "    ASSAY_TO_COMPOUNDS[(assay_id, activity_type, unit)].add(compound_chembl_id)\n",
    "del ChEMBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711df9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded qt: 26370 files\n",
      "Loaded ql: 1536 files\n"
     ]
    }
   ],
   "source": [
    "def load_all_gz_csvs_from_zip(zip_path: str) -> dict[str, pd.DataFrame]:\n",
    "    dfs = {}\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        for name in z.namelist():\n",
    "            if name.endswith(\".csv.gz\"):\n",
    "                with z.open(name) as f:\n",
    "                    dfs[name] = pd.read_csv(f, compression=\"gzip\")\n",
    "    return dfs\n",
    "\n",
    "datasets_dir = os.path.join(OUTPUT, pathogen_code, \"datasets\")\n",
    "\n",
    "qt_zip = os.path.join(datasets_dir, \"datasets_qt.zip\")\n",
    "ql_zip = os.path.join(datasets_dir, \"datasets_ql.zip\")\n",
    "\n",
    "dfs_qt = load_all_gz_csvs_from_zip(qt_zip)\n",
    "dfs_ql = load_all_gz_csvs_from_zip(ql_zip)\n",
    "\n",
    "print(\"Loaded qt:\", len(dfs_qt), \"files\")\n",
    "print(\"Loaded ql:\", len(dfs_ql), \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bca2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137607"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([j for i in ASSAY_TO_COMPOUNDS for j in ASSAY_TO_COMPOUNDS[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08304327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10532, 10532, 10532)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ASSAYS_CLEANED), len(ASSAYS_PARAMETERS), len(ASSAYS_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05875438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_COLS = [\"assay_id\", \"assay_type\", \"assay_organism\", \"target_organism\", \"organism_curated\", \"doc_chembl_id\", \"target_type\", \"target_type_curated\", \"target_type_curated_extra\", \n",
    "          \"target_chembl_id\", \"target_chembl_id_curated\", \"target_name_curated\", \"bao_label\", \"source_label\", \"strain\", \"atcc_id\", \"mutations\", \"known_drug_resistances\", \"media\",\n",
    "          \"activity_type\", \"unit\", \"activities\", \"nan_values\", \"cpds\", \"frac_cs\", \"direction\", \"act_flag\", 'inact_flag', \"equal\", \"higher\", \"lower\", \"dataset_type\", \"cutoffs\", \"ratios\", \n",
    "          \"cpds_qt\", \"pos_ql\", \"ratio_ql\", \"cpds_ql\", \"min_\", \"p1\", \"p25\", \"p50\", \"p75\", \"p99\", \"max_\", 'Accepted', 'Considered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d57f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_results_from_individual_modeling(LABELS):\n",
    "    RESULTS, CONSIDERED_ASSAYS = {}, {}\n",
    "    for LABEL in LABELS:\n",
    "        RESULTS[LABEL] = {}\n",
    "        CONSIDERED_ASSAYS[LABEL] = set()\n",
    "        rows = INDIVIDUAL_LM[INDIVIDUAL_LM[LABEL]][[\"assay_id\", \"activity_type\", \"unit\", \"expert_cutoff\", f\"{LABEL}_AVG\"]].values\n",
    "        for assay_id, activity_type, unit, expert_cutoff, auroc in rows:\n",
    "            key = (assay_id, activity_type, unit)\n",
    "            CONSIDERED_ASSAYS[LABEL].add(key)\n",
    "            if auroc > 0.7:\n",
    "                if key not in RESULTS[LABEL]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "                elif auroc > RESULTS[LABEL][key][1]:\n",
    "                    RESULTS[LABEL][key] = [expert_cutoff, auroc]\n",
    "    return RESULTS, CONSIDERED_ASSAYS\n",
    "\n",
    "def where_considered(key, LABELS, CONSIDERED_ASSAYS):\n",
    "    considered = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in CONSIDERED_ASSAYS[LABEL]:\n",
    "            considered.append(LABEL)\n",
    "    if len(considered) > 0:\n",
    "        return \";\".join(considered)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def where_accepted(key, LABELS, ACCEPTED_ASSAYS):\n",
    "    accepted = []\n",
    "    for LABEL in LABELS:\n",
    "        if key in ACCEPTED_ASSAYS[LABEL]:\n",
    "            accepted.append(LABEL)\n",
    "    if len(accepted) > 0:\n",
    "        return \";\".join(accepted)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Get results from individual modeling ABCD\n",
    "LABELS = ['A', 'B', 'C', 'D']\n",
    "ACCEPTED_ASSAYS, CONSIDERED_ASSAYS = get_all_results_from_individual_modeling(LABELS)\n",
    "\n",
    "col_accepted, col_considered = [], []\n",
    "for assay_id, activity_type, unit in ASSAYS_MASTER[[\"assay_id\", \"activity_type\", \"unit\"]].values:\n",
    "    # Get strategies in which this assay is considered and accepted\n",
    "    key = tuple([assay_id, activity_type, unit])\n",
    "    col_considered.append(where_considered(key, LABELS, CONSIDERED_ASSAYS))\n",
    "    col_accepted.append(where_accepted(key, LABELS, ACCEPTED_ASSAYS))\n",
    "ASSAYS_MASTER['Accepted'] = col_accepted\n",
    "ASSAYS_MASTER['Considered'] = col_considered\n",
    "ASSAYS_MASTER = ASSAYS_MASTER[ALL_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3f82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_assays = ASSAYS_MASTER[(ASSAYS_MASTER['Accepted'].isna() == False)][['assay_id', 'activity_type', 'unit']].values\n",
    "accepted_compounds = set([j for i in accepted_assays for j in ASSAY_TO_COMPOUNDS[tuple(i)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0bb2cece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97590"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accepted_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4588e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_ASSAYS_MASTER = ASSAYS_MASTER[(ASSAYS_MASTER['Considered'].isna()) & (ASSAYS_MASTER['target_type_curated_extra'] == 'ORGANISM')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be7d1b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_type</th>\n",
       "      <th>unit</th>\n",
       "      <th>target_type_curated_extra</th>\n",
       "      <th>bao_label</th>\n",
       "      <th>strain</th>\n",
       "      <th>n_cpds_red</th>\n",
       "      <th>n_assays</th>\n",
       "      <th>cum_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>MIC</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>25055</td>\n",
       "      <td>1948</td>\n",
       "      <td>0.321717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>MIC</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6228</td>\n",
       "      <td>917</td>\n",
       "      <td>0.401687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>MIC90</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>4766</td>\n",
       "      <td>284</td>\n",
       "      <td>0.462885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>INHIBITION</td>\n",
       "      <td>%</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>2857</td>\n",
       "      <td>314</td>\n",
       "      <td>0.499570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>MIC99</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>2510</td>\n",
       "      <td>134</td>\n",
       "      <td>0.531799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>MIC</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Ra</td>\n",
       "      <td>2323</td>\n",
       "      <td>218</td>\n",
       "      <td>0.561628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>MIC90</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813</td>\n",
       "      <td>157</td>\n",
       "      <td>0.584907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>GI</td>\n",
       "      <td>%</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>1629</td>\n",
       "      <td>178</td>\n",
       "      <td>0.605824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>IC50</td>\n",
       "      <td>umol.L-1</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>1286</td>\n",
       "      <td>115</td>\n",
       "      <td>0.622337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>INHIBITION</td>\n",
       "      <td>%</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1269</td>\n",
       "      <td>67</td>\n",
       "      <td>0.638632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ACTIVITY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ORGANISM</td>\n",
       "      <td>organism-based format</td>\n",
       "      <td>H37Rv</td>\n",
       "      <td>1110</td>\n",
       "      <td>407</td>\n",
       "      <td>0.652885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     activity_type      unit target_type_curated_extra              bao_label strain  n_cpds_red  n_assays  cum_prop\n",
       "655            MIC  umol.L-1                  ORGANISM  organism-based format  H37Rv       25055      1948  0.321717\n",
       "1209           MIC  umol.L-1                  ORGANISM  organism-based format    NaN        6228       917  0.401687\n",
       "1329         MIC90  umol.L-1                  ORGANISM  organism-based format  H37Rv        4766       284  0.462885\n",
       "266     INHIBITION         %                  ORGANISM  organism-based format  H37Rv        2857       314  0.499570\n",
       "1420         MIC99  umol.L-1                  ORGANISM  organism-based format  H37Rv        2510       134  0.531799\n",
       "651            MIC  umol.L-1                  ORGANISM  organism-based format  H37Ra        2323       218  0.561628\n",
       "1383         MIC90  umol.L-1                  ORGANISM  organism-based format    NaN        1813       157  0.584907\n",
       "164             GI         %                  ORGANISM  organism-based format  H37Rv        1629       178  0.605824\n",
       "221           IC50  umol.L-1                  ORGANISM  organism-based format  H37Rv        1286       115  0.622337\n",
       "284     INHIBITION         %                  ORGANISM  organism-based format    NaN        1269        67  0.638632\n",
       "95        ACTIVITY       NaN                  ORGANISM  organism-based format  H37Rv        1110       407  0.652885"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = [\"activity_type\", \"unit\", \"target_type_curated_extra\", \"bao_label\", \"strain\"]\n",
    "\n",
    "cpds_grouped = (FILTERED_ASSAYS_MASTER\n",
    "    .groupby(keys, dropna=False)\n",
    "    .agg(n_cpds_red=(\"cpds\", \"sum\"), n_assays=(\"assay_id\", \"size\"))\n",
    "    .reset_index()\n",
    "    .sort_values(\"n_cpds_red\", ascending=False)\n",
    ")\n",
    "\n",
    "cpds_grouped[\"cum_prop\"] = cpds_grouped[\"n_cpds_red\"].cumsum() / cpds_grouped[\"n_cpds_red\"].sum()\n",
    "cpds_grouped = cpds_grouped[cpds_grouped['n_cpds_red'] >= 1000]\n",
    "cpds_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94231d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_assay_master(activity_type, unit, target_type_curated_extra, bao_label, strain):\n",
    "    if type(unit) == str:\n",
    "        df = FILTERED_ASSAYS_MASTER[(FILTERED_ASSAYS_MASTER['activity_type'] == activity_type) & \n",
    "                    (FILTERED_ASSAYS_MASTER['unit'] == unit) &\n",
    "                    (FILTERED_ASSAYS_MASTER['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (FILTERED_ASSAYS_MASTER['bao_label'] == bao_label) &\n",
    "                    (FILTERED_ASSAYS_MASTER['strain'] == strain)]\n",
    "    else:\n",
    "        df = FILTERED_ASSAYS_MASTER[(FILTERED_ASSAYS_MASTER['activity_type'] == activity_type) & \n",
    "                    (FILTERED_ASSAYS_MASTER['unit'].isna()) &\n",
    "                    (FILTERED_ASSAYS_MASTER['target_type_curated_extra'] == target_type_curated_extra) &\n",
    "                    (FILTERED_ASSAYS_MASTER['bao_label'] == bao_label) &\n",
    "                    (FILTERED_ASSAYS_MASTER['strain'] == strain)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_expert_cutoffs(CONFIGPATH):\n",
    "    \"\"\"\n",
    "    Load expert cutoffs from the manual curation CSV and return them as a dictionary.\n",
    "\n",
    "    The CSV is expected at:\n",
    "        {CONFIGPATH}/manual_curation/expert_cutoffs.csv\n",
    "\n",
    "    The returned dictionary maps:\n",
    "        (activity_type, unit, target_type, pathogen_code) -> expert_cutoff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    CONFIGPATH : str\n",
    "        Path to the config folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of expert cutoffs keyed by\n",
    "        (activity_type, unit, target_type, pathogen_code).\n",
    "    \"\"\"\n",
    "    # Load expert cut-offs\n",
    "    EXPERT_CUTOFFS = pd.read_csv(os.path.join(CONFIGPATH, \"expert_cutoffs.csv\"))\n",
    "\n",
    "    EXPERT_CUTOFFS = {\n",
    "        (a, b, c, d): [float(k) for k in e.split(\";\")]\n",
    "        for a, b, c, d, e in EXPERT_CUTOFFS[\n",
    "            [\"activity_type\", \"unit\", \"target_type\", \"pathogen_code\", \"expert_cutoff\"]\n",
    "        ].values\n",
    "    }\n",
    "\n",
    "    return EXPERT_CUTOFFS\n",
    "\n",
    "def load_ecfp_all(h5_path):\n",
    "    \"\"\"Load all ECFP (Morgan count) fingerprints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing datasets \"SMILES\" and \"X_morgan\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, np.ndarray]\n",
    "        Mapping {chembl_id: fingerprint (np.int8, shape (nBits,))}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        meta = f[\"SMILES\"][:, 3].astype(str)\n",
    "        fps  = f[\"X_morgan\"][:]  # Load ALL\n",
    "\n",
    "    return {cid: fp for cid, fp in zip(meta, fps)}\n",
    "\n",
    "def KFoldTrain(X, Y, n_splits=4, n_estimators=100, random_state=42):\n",
    "    \"\"\"Stratified K-fold training/eval with RandomForest; returns mean AUROC and std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Binary labels (n_samples,).\n",
    "    n_splits : int\n",
    "        Number of folds.\n",
    "    n_estimators : int\n",
    "        Number of trees in the random forest.\n",
    "    random_state : int\n",
    "        RNG seed (also used for fold shuffling).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        (mean_auroc, std_auroc) rounded to 3 decimals.\n",
    "    \"\"\"\n",
    "    def init_RF():\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=8,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    aurocs = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = init_RF()\n",
    "        rf.fit(X_train, Y_train)\n",
    "        y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "        aurocs.append(roc_auc_score(Y_test, y_prob))\n",
    "\n",
    "    return round(float(np.mean(aurocs)), 3), round(float(np.std(aurocs)), 3)\n",
    "\n",
    "def TrainRF(X, Y, n_estimators=100):\n",
    "    \"\"\"Train a RandomForestClassifier on all provided data and return the fitted model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n_samples, n_features).\n",
    "    Y : np.ndarray\n",
    "        Labels (n_samples,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RandomForestClassifier\n",
    "        Fitted classifier.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=8,\n",
    "    )\n",
    "    rf.fit(X, Y)\n",
    "    return rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8f9a818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expert cut-offs\n",
    "EXPERT_CUTOFFS = load_expert_cutoffs(CONFIGPATH)\n",
    "\n",
    "# Loading Morgan fingerprints\n",
    "PATH_TO_ECFPs = os.path.join(DATAPATH, \"chembl_processed\", \"ChEMBL_ECFPs.h5\")\n",
    "ecfps = load_ecfp_all(PATH_TO_ECFPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "581f3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 5.0\n",
      "\tCompounds: 18155 Positives: 5000 (27.5%)\n",
      "\tMean AUROC: 0.898 ± 0.004\n",
      "10.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 10.0\n",
      "\tCompounds: 18155 Positives: 6738 (37.1%)\n",
      "\tMean AUROC: 0.889 ± 0.007\n",
      "20.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 20.0\n",
      "\tCompounds: 18155 Positives: 8915 (49.1%)\n",
      "\tMean AUROC: 0.89 ± 0.004\n",
      "5.0\n",
      "Merging ... Activity type: MIC90, Unit: umol.L-1, Cutoff: 5.0\n",
      "\tCompounds: 3216 Positives: 1391 (43.3%)\n",
      "\tMean AUROC: 0.929 ± 0.009\n",
      "10.0\n",
      "Merging ... Activity type: MIC90, Unit: umol.L-1, Cutoff: 10.0\n",
      "\tCompounds: 3216 Positives: 1625 (50.5%)\n",
      "\tMean AUROC: 0.938 ± 0.008\n",
      "20.0\n",
      "Merging ... Activity type: MIC90, Unit: umol.L-1, Cutoff: 20.0\n",
      "\tCompounds: 3216 Positives: 1943 (60.4%)\n",
      "\tMean AUROC: 0.937 ± 0.007\n",
      "25.0\n",
      "Merging ... Activity type: INHIBITION, Unit: %, Cutoff: 25.0\n",
      "\tCompounds: 2176 Positives: 1620 (74.4%)\n",
      "\tMean AUROC: 0.863 ± 0.017\n",
      "50.0\n",
      "Merging ... Activity type: INHIBITION, Unit: %, Cutoff: 50.0\n",
      "\tCompounds: 2176 Positives: 1286 (59.1%)\n",
      "\tMean AUROC: 0.852 ± 0.014\n",
      "75.0\n",
      "Merging ... Activity type: INHIBITION, Unit: %, Cutoff: 75.0\n",
      "\tCompounds: 2176 Positives: 944 (43.4%)\n",
      "\tMean AUROC: 0.852 ± 0.018\n",
      "5.0\n",
      "Merging ... Activity type: MIC99, Unit: umol.L-1, Cutoff: 5.0\n",
      "\tCompounds: 1619 Positives: 583 (36.0%)\n",
      "\tMean AUROC: 0.908 ± 0.016\n",
      "10.0\n",
      "Merging ... Activity type: MIC99, Unit: umol.L-1, Cutoff: 10.0\n",
      "\tCompounds: 1619 Positives: 737 (45.5%)\n",
      "\tMean AUROC: 0.91 ± 0.022\n",
      "20.0\n",
      "Merging ... Activity type: MIC99, Unit: umol.L-1, Cutoff: 20.0\n",
      "\tCompounds: 1619 Positives: 920 (56.8%)\n",
      "\tMean AUROC: 0.897 ± 0.012\n",
      "5.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 5.0\n",
      "\tCompounds: 1935 Positives: 313 (16.2%)\n",
      "\tMean AUROC: 0.864 ± 0.025\n",
      "10.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 10.0\n",
      "\tCompounds: 1935 Positives: 475 (24.5%)\n",
      "\tMean AUROC: 0.878 ± 0.014\n",
      "20.0\n",
      "Merging ... Activity type: MIC, Unit: umol.L-1, Cutoff: 20.0\n",
      "\tCompounds: 1935 Positives: 661 (34.2%)\n",
      "\tMean AUROC: 0.895 ± 0.014\n",
      "25.0\n",
      "Merging ... Activity type: GI, Unit: %, Cutoff: 25.0\n",
      "\tCompounds: 1144 Positives: 782 (68.4%)\n",
      "\tMean AUROC: 0.905 ± 0.018\n",
      "50.0\n",
      "Merging ... Activity type: GI, Unit: %, Cutoff: 50.0\n",
      "\tCompounds: 1144 Positives: 680 (59.4%)\n",
      "\tMean AUROC: 0.903 ± 0.016\n",
      "75.0\n",
      "Merging ... Activity type: GI, Unit: %, Cutoff: 75.0\n",
      "\tCompounds: 1144 Positives: 555 (48.5%)\n",
      "\tMean AUROC: 0.908 ± 0.017\n",
      "5.0\n",
      "Merging ... Activity type: IC50, Unit: umol.L-1, Cutoff: 5.0\n",
      "\tCompounds: 1048 Positives: 405 (38.6%)\n",
      "\tMean AUROC: 0.878 ± 0.024\n",
      "10.0\n",
      "Merging ... Activity type: IC50, Unit: umol.L-1, Cutoff: 10.0\n",
      "\tCompounds: 1048 Positives: 526 (50.2%)\n",
      "\tMean AUROC: 0.879 ± 0.009\n",
      "20.0\n",
      "Merging ... Activity type: IC50, Unit: umol.L-1, Cutoff: 20.0\n",
      "\tCompounds: 1048 Positives: 638 (60.9%)\n",
      "\tMean AUROC: 0.887 ± 0.019\n",
      "Too few data for ACTIVITY, nan, ORGANISM, organism-based format, H37Rv... (1110 --> 653 compounds) after merging\n"
     ]
    }
   ],
   "source": [
    "merged_compounds = []\n",
    "\n",
    "for merging in cpds_grouped.itertuples():\n",
    "\n",
    "    # Get data\n",
    "    activity_type = merging.activity_type\n",
    "    unit = merging.unit\n",
    "    target_type_curated_extra = merging.target_type_curated_extra\n",
    "    bao_label = merging.bao_label\n",
    "    strain = merging.strain\n",
    "\n",
    "    # Filter master table\n",
    "    df = get_filtered_assay_master(activity_type, unit, target_type_curated_extra, bao_label, strain)\n",
    "\n",
    "    # Get quantitative and qualitative\n",
    "    df_quant = df[(df['dataset_type'] == 'quantitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "    df_qual = df[(df['dataset_type'] == 'qualitative') | (df['dataset_type'] == 'mixed')].reset_index(drop=True)\n",
    "\n",
    "    # Quantitative\n",
    "    if len(df_quant) > 0 and sum(df_quant['cpds']) > 1000:\n",
    "\n",
    "        # For each expert cut-off\n",
    "        for expert_cutoff in EXPERT_CUTOFFS[(activity_type, unit, target_type_curated_extra, pathogen_code)]:\n",
    "            print(expert_cutoff)\n",
    "            assays = df_quant['assay_id'].tolist()\n",
    "            files = [f\"{i}_{activity_type}_{unit}_qt_{expert_cutoff}.csv.gz\" for i in assays]\n",
    "            data = [dfs_qt[f].assign(assay_id=a) for a, f in zip(assays, files)]\n",
    "            data = pd.concat(data, ignore_index=True)\n",
    "            data = data.sort_values(\"value\", ascending=True).drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "            if len(data) > 1000:\n",
    "                X = np.array(data['compound_chembl_id'].map(ecfps).to_list())\n",
    "                Y = np.array(data['bin'].tolist())\n",
    "                if sum(Y) > 50 and sum(Y) / len(Y):\n",
    "                    print(f\"Merging ... Activity type: {activity_type}, Unit: {unit}, Cutoff: {expert_cutoff}\")\n",
    "                    print(f\"\\tCompounds: {len(X)}\", f\"Positives: {sum(Y)} ({round(100 * sum(Y) / len(Y), 1)}%)\")\n",
    "                    # 4Fold Cros Validation\n",
    "                    average_auroc, stds = KFoldTrain(X, Y, n_splits=5, n_estimators=100)\n",
    "                    print(f\"\\tMean AUROC: {average_auroc} ± {stds}\")\n",
    "                    if average_auroc > 0.7:\n",
    "                        merged_compounds.extend(data['compound_chembl_id'].tolist())\n",
    "\n",
    "                    # # If performance is good enough, train on full data and predict on reference set\n",
    "                    # if average_auroc > 0.7:\n",
    "                    #     RF = TrainRF(X, Y, n_estimators=100)\n",
    "                    #     y_prob_ref = RF.predict_proba(X_REF)[:, 1]\n",
    "                    #     os.makedirs(os.path.join(PATH_TO_CORRELATIONS, LABEL), exist_ok=True)\n",
    "                    #     np.savez_compressed(os.path.join(PATH_TO_CORRELATIONS, LABEL, filename.replace(\".csv.gz\", \"_ref_probs.npz\")), y_prob_ref=y_prob_ref)\n",
    "\n",
    "\n",
    "    if len(df_qual) > 0 and sum(df_qual['cpds']) > 1000:\n",
    "\n",
    "        # Get assays, files and data\n",
    "        assays = df_qual['assay_id'].tolist()\n",
    "        files = [f\"{i}_{activity_type}_{unit}_ql.csv.gz\" for i in assays]\n",
    "        data = [dfs_ql[file] for file in files]\n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "        data = data.sort_values(\"bin\").drop_duplicates(\"compound_chembl_id\", keep=\"first\").reset_index(drop=True)\n",
    "        if len(data) > 1000:\n",
    "            ...\n",
    "        else:\n",
    "            print(f\"Too few data for {activity_type}, {unit}, {target_type_curated_extra}, {bao_label}, {strain}... ({sum(df['cpds'])} --> {len(data)} compounds) after merging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "299851f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26134"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_compounds = set(merged_compounds)\n",
    "len(merged_compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9cf77c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25903"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in merged_compounds if i not in accepted_compounds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "882331ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123493"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_compounds.union(accepted_compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "060f6758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974325434025885"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123493/ 137607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd62a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a1258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62de8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5323327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bb253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd0e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb44696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa44997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535e270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c2ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38010f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f1919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201e788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c63db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"strain\", \"atcc_id\", \"media\", \"mutations\", \"known_drug_resistances\"]\n",
    "df[cols].isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95973590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fef840",
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_weight = 0.5\n",
    "atcc_id_weight = 0.2\n",
    "media_weight = 0.1\n",
    "mutations_weight = 0.1\n",
    "known_drug_resistances_weight = 0.1\n",
    "\n",
    "# Normalize fields\n",
    "df[\"strain\"]  = df[\"strain\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "df[\"atcc_id\"] = df[\"atcc_id\"].fillna(\"\").astype(str).str.strip().str.upper()\n",
    "df[\"media\"] = df[\"media\"].fillna(\"\").astype(str).str.strip().str.upper()\n",
    "df[\"mutations\"] = df[\"mutations\"].fillna(\"\").astype(str).str.strip().str.upper()\n",
    "df[\"known_drug_resistances\"] = df[\"known_drug_resistances\"].fillna(\"\").astype(str).str.strip().str.upper()\n",
    "\n",
    "# Map node to info\n",
    "df[\"_node\"] = list(zip(df[\"assay_id\"], df[\"activity_type\"], df[\"unit\"]))\n",
    "node_to_atcc = dict(zip(df[\"_node\"], df[\"atcc_id\"]))\n",
    "node_to_media = dict(zip(df[\"_node\"], df[\"media\"]))\n",
    "node_to_mutations = dict(zip(df[\"_node\"], df[\"mutations\"]))\n",
    "node_to_known_drug_resistances = dict(zip(df[\"_node\"], df[\"known_drug_resistances\"]))\n",
    "\n",
    "def to_set(s):\n",
    "    s = \"\" if pd.isna(s) else str(s).strip().upper()\n",
    "    if s == \"\":\n",
    "        return set()\n",
    "    return {x.strip() for x in s.split(\";\") if x.strip()}\n",
    "\n",
    "node_to_mutations = {n: to_set(m) for n, m in zip(df[\"_node\"], df[\"mutations\"])}\n",
    "node_to_known_drug_resistances = {n: to_set(m) for n, m in zip(df[\"_node\"], df[\"known_drug_resistances\"])}\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (assay_id, activity_type, unit)\n",
    "for r in df.itertuples(index=False):\n",
    "    G.add_node((r.assay_id, r.activity_type, r.unit))\n",
    "\n",
    "# Add edges: connect assays with the same strain\n",
    "for strain, sub in df[df[\"strain\"] != \"\"].groupby(\"strain\"):\n",
    "\n",
    "    # Unique nodes in this strain\n",
    "    nodes = list({(r.assay_id, r.activity_type, r.unit) for r in sub.itertuples(index=False)})\n",
    "\n",
    "    # Connect all pairs having the same strain\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i + 1, len(nodes)):\n",
    "            G.add_edge(nodes[i], nodes[j], weight_strain=strain_weight)\n",
    "\n",
    "print(\"Nodes:\", G.number_of_nodes(), \"Same strain edges:\", G.number_of_edges())\n",
    "\n",
    "# ATCC ID\n",
    "n_with_atcc_id = (df[\"atcc_id\"] != \"\").sum()\n",
    "print(\"Nodes with ATCC ID:\", n_with_atcc_id, \"from\", len(df), f\"({100*n_with_atcc_id/len(df):.2f}%)\")\n",
    "boosted_atcc = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    a = node_to_atcc[u]\n",
    "    b = node_to_atcc[v]\n",
    "    if a != \"\" and b != \"\" and a == b:\n",
    "        data[\"weight_atcc_id\"] = atcc_id_weight\n",
    "        boosted_atcc += 1\n",
    "    elif (a == \"\" and b != \"\") or (a != \"\" and b == \"\"):\n",
    "        data[\"weight_atcc_id\"] = atcc_id_weight / 2\n",
    "        boosted_atcc += 1\n",
    "    else:\n",
    "        data[\"weight_atcc_id\"] = 0\n",
    "print(f\"ATCC-boosted edges: {boosted_atcc} from {G.number_of_edges()} ({100*round(boosted_atcc/G.number_of_edges(), 4)}%)\")\n",
    "\n",
    "# MEDIA\n",
    "n_with_media = (df[\"media\"] != \"\").sum()\n",
    "print(\"Nodes with media:\", n_with_media, \"from\", len(df), f\"({100*n_with_media/len(df):.2f}%)\")\n",
    "boosted_media = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    a = node_to_media[u]\n",
    "    b = node_to_media[v]\n",
    "    if a != \"\" and b != \"\" and a == b:\n",
    "        data[\"weight_media\"] = media_weight\n",
    "        boosted_media += 1\n",
    "    # elif (a == \"\" and b != \"\") or (a != \"\" and b == \"\"):\n",
    "    #     data[\"weight_media\"] = media_weight / 2\n",
    "    #     boosted_media += 1\n",
    "    else:\n",
    "        data[\"weight_media\"] = 0\n",
    "print(f\"MEDIA-boosted edges: {boosted_media} from {G.number_of_edges()} ({100*boosted_media/G.number_of_edges():.2f}%)\")\n",
    "\n",
    "\n",
    "boosted_mut = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    a = node_to_mutations[u]\n",
    "    b = node_to_mutations[v]\n",
    "\n",
    "    if len(a) > 0 and len(b) > 0:\n",
    "        sim = len(a & b) / len(a | b)\n",
    "        data[\"weight_mutations\"] = mutations_weight * sim\n",
    "        boosted_mut += 1 if sim > 0 else 0\n",
    "    # elif len(a) == 0 and len(b) == 0:\n",
    "    #     data[\"weight_mutations\"] = mutations_weight / 2\n",
    "    #     boosted_mut += 1\n",
    "    else:\n",
    "        data[\"weight_mutations\"] = 0\n",
    "\n",
    "print(f\"MUTATIONS-boosted edges: {boosted_mut} from {G.number_of_edges()} ({100*boosted_mut/G.number_of_edges():.2f}%)\")\n",
    "\n",
    "boosted_kdr = 0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    a = node_to_known_drug_resistances[u]\n",
    "    b = node_to_known_drug_resistances[v]\n",
    "\n",
    "    if len(a) > 0 and len(b) > 0:\n",
    "        sim = len(a & b) / len(a | b) \n",
    "        data[\"weight_known_drug_resistances\"] = known_drug_resistances_weight * sim\n",
    "        boosted_kdr += 1 if sim > 0 else 0\n",
    "    else:\n",
    "        data[\"weight_known_drug_resistances\"] = 0.0\n",
    "\n",
    "print(f\"KDR-boosted edges: {boosted_kdr} from {G.number_of_edges()} ({100*boosted_kdr/G.number_of_edges():.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final weight for every edge\n",
    "for _, _, data in G.edges(data=True):\n",
    "    data[\"weight\"] = (\n",
    "        data[\"weight_strain\"] +\n",
    "        data[\"weight_atcc_id\"] +\n",
    "        data[\"weight_media\"] +\n",
    "        data[\"weight_mutations\"] +\n",
    "        data[\"weight_known_drug_resistances\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = 0.7\n",
    "\n",
    "G_thr = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d.get(\"weight\", 0.0) >= THR)\n",
    "components = [c for c in nx.connected_components(G_thr) if len(c) > 1]\n",
    "components = sorted(components, key=len, reverse=True)\n",
    "\n",
    "print(\"n_components:\", len(components))\n",
    "print(\"top sizes:\", [len(c) for c in components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nunique_nonempty(x):\n",
    "    x = pd.Series(x).dropna()\n",
    "    x = x[x != \"\"]\n",
    "    return x.nunique()\n",
    "\n",
    "def is_consistent(group_df):\n",
    "    return (\n",
    "        nunique_nonempty(group_df[\"atcc_id\"]) <= 1 and\n",
    "        nunique_nonempty(group_df[\"media\"])   <= 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistent_groups = []\n",
    "inconsistent_groups = []\n",
    "\n",
    "for comp in components:\n",
    "    sub = df[df[\"_node\"].isin(comp)].copy()\n",
    "    if is_consistent(sub):\n",
    "        consistent_groups.append(sub)\n",
    "    else:\n",
    "        inconsistent_groups.append(sub)\n",
    "\n",
    "print(\"consistent groups:\", len(consistent_groups))\n",
    "print(\"inconsistent groups:\", len(inconsistent_groups))\n",
    "print(\"top consistent sizes:\", sorted([len(g) for g in consistent_groups], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the biggest consistent group\n",
    "g = consistent_groups[0]\n",
    "assay_ids = g[\"assay_id\"].unique()\n",
    "print(\"group size:\", len(g), \"assays:\", len(assay_ids))\n",
    "print(\"ATCC non-empty uniques:\", g.loc[g[\"atcc_id\"]!=\"\",\"atcc_id\"].nunique())\n",
    "print(\"media non-empty uniques:\", g.loc[g[\"media\"]!=\"\",\"media\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
